\chapter{Metodología}
\label{chap:metho}

\section{La maquinaria por debajo}
\label{sec:maquinaria}
Para lograr el objetivo general, en la metodología se propone utilizar el modelo NMS de Geyer~\cite{Geyer2010} extendido en~\cite{Song2015,Dzeladini2014}, como representación de la planta real del humanoide a estudiar. Existe varias ventajas de tomar ese modelo: (1) Desde el punto de vista de la computación morfológica, la complejidad de la señal de control, como se demostró en~\cite{Ghazi-Zahedi2015a}, permite simplificar el espacio del estado-acción, imponiendo además restricciones naturales presentes en el cuerpo humano y explotando la riqueza dinámica natural de la auto-estabilización de algunos comportamientos mediante la \emph{mórfosis} del sistema; (2) La implementación de diferentes tipos de retardos sobre diferentes \emph{pathways}, permite formar acciones de control reactivas y otras de control predictivas, estas últimas requieren de un mayor cómputo y un mayor tiempo, que son necesarios para tomar una decisión más elaborada. Esta característica constituye el fundamento morfológico de un control jerárquico; (3) Aprovechando el modelo extendido de~\cite{Song2015}, la estructura jerárquica permite diferentes tipos de movimientos como caminar, correr y transiciones de velocidades. Los tres bajo diferentes tipos de terrenos. Estos serán ejemplos demostrativos de movimiento, útiles para el aprendizaje; (4) Aprovechando el modelo extendido de~\cite{Dzeladini2014} de la capa de CPGs, donde se permite modular los reflejos y formar predicciones de la interacción, además de guardar dichas configuraciones de los CPGs como modelos predictivos. Esto será importante para practicar el \emph{ensayo mental}\footnote{mental rehearsal} y dar una forma estructurada a los modelos internos. 

La implementación del modelo de Geyer, propuesto en~\cite{Dzeladini2014}, tiene una librería del sistema NMS desarrollada en C++ y es fácil acoplar a un motor físico de dinámica multicuerpo. En este caso para el motor físico, las librerías RDBL desarrolladas por~\cite{Felis2016}, de los algoritmos de~\cite{Featherstone2008} que incluyen un modelo de contacto e impacto con fricción~\cite{Uchida2015}, desarrolladas en C++ con extensión a Python a través de Cython, son escogidas por la versatilidad del código y su extensión para la optimización con GPU mediante CUDA a traves de Eigen 3~\cite{Guennebaud2010}.

% modelo de friccion, por que python?, por que C++, por que CUDA? visualizacion como post-proceso, OpenAI Gym~\cite{Brockman2016}

% NOVEDAD: dirección sobre el modelo NMS, musculos en adicionales del pie
% NOVEDAD: usar DMPs en las restricciones virtuales en el controlador HZD
% NOVEDAD: modelos de D-SLIP, SLIP y LIPM con HZD
% NOVEDAD: exploración de la representación de características en la función de recompensa.

%Probability is about modeling uncertainty in the world.
%Inference is about learning properties of the world from data that we observe.
%Computation is needed to automate probabilistic analysis and inference, especially to handle massive data sets.
%Sterotypical movements enable prediction and statistics.
%For every movement skill, we associate motor and perceptual variables and theit statistics over time.
%This information, represented in memory as ASM, can be used in error corretion, pattern completion, switching and abort behaviors 

\section{La piedra angular: Búsqueda de Políticas y Costos}
\label{sec:piedra}
Para la búsqueda de la políticas mediante RL Robótico, configurar el algoritmo adecuado de RRL, requiere de pruebas con diferentes elecciones. Por el problema específico del humanoide acoplado a un sistema NMS, solo al final de las pruebas se sabrá cuál combinación es la más adecuada. Por lo tanto especificar la configuración del algoritmo de RRL, es un resultado que debe ser encontrado. La representación de la política será mediante redes neuronales (ANNs), primitivas de movimiento dinámicas (DMPs)\footnote{Las PMP también son una opción que se estudiará} y Osciladores morfados (MOs), estos últimos~\cite{Ajallooeian2013} son una generalización de los DMPs rítmicos y los CPGs. Un algoritmo RRL tiene por lo general tres ingredientes: un paso de evaluación de políticas, otro paso de mejoramiento o actualización de políticas y un método de exploración. Es posible configurar dichos ingredientes utilizando diferentes estrategias~\cite{Kober2013,Deisenroth2013}. Los tres ingredientes serán claramente estudiados, teniendo en cuenta: (1) Por \emph{evaluación de la política}, donde la evaluación puede ser cada paso o cada episodio, o donde se puede utilizar las técnicas de \emph{diferencias temporales} (TD); (2) Por \emph{actualización de la política}, en donde se debe elegir entre algoritmos basados en: el \emph{gradiente de la política}, el algoritmo EM\footnote{Expectación-Maximización}, las métricas de Información (IT), las integrales de camino ($PI^2$) y la optimización estocástica; y la última (3) Por \emph{estrategia de exploración}, que tiene aún más posibilidades, las cuales pueden funcionar en conjunto, dando múltiples combinaciones posibles. Estas son: (i) \emph{espacio de la perturbación}, sobre el espacio de acción o el espacio de parámetros, en el último se puede estructurar el ruido y trabajar un nivel superior de aprendizaje de política; (ii) \emph{escala de tiempo}, nuevamente esta por paso o por episodio, en el primero se puede evitar los mínimos locales y en el segundo, las actualizaciones de las políticas resultan más confiables; (iii) \emph{distribución de ruido}, en donde se usa matrices de correlación basadas en métodos de segundo orden, acá el método de \emph{Covariance Matrix Adaptation} (CMA) será estudiado por sus resultados; y (iv) \emph{actualización de la distribución del ruido} donde se puede variar las tasas de exploración y utilizar métricas de entropía relativa a medida que itera el algoritmo.

En cuanto al problema de volver el RRL tratable, los retos en la metodología son: 
\begin{enumerate}
\item El \emph{problema de la dimensionalidad} que se enfrentará mediante tres ideas: (1) la representación de políticas parametrizadas y estructuradas, utilizando MOs para movimientos rítmicos y DMPs para movimientos discretos, aplicados al nivel de activación muscular como representación de las políticas; (2) las sinergias musculares propuestas y la capacidad de cómputo del MC en el modelo NMS presentado en~\cite{Song2015} como la reducción de la dimensionalidad, la simplificación de la señal de control y el manejo de variables latentes; y (3) la búsqueda local partiendo de ejemplos demostrativos. 
\item La \emph{exploración cuidadosa}, mediante las cuatro opciones de estrategias de exploración descritas anteriormente, busca políticas que no sean nocivas o destructivas para la estructura del humanoide. El punto clave aquí, es una exploración local que también contribuye para tratar el problema de la dimensionalidad.
\item La \emph{eficiencia estadística}. En este punto el manejo eficiente de grandes cantidades de datos, procedentes de la información del lazo sensoriomotor al interactuar, se logra de la formación de modelos probabilísticos internos, que representan modelos directos locales, estimadores de estado, modelos de recompensa, predicciones de fallos ante perturbaciones y otros más, que permiten la capacidad cognitiva de realizar ensayos metales produciendo un manejo eficiente de los datos y una eficiencia estadística que reduce el número de muestras. El uso de \emph{mini-batches} y un manejo adecuado de los datos de muestra, permite aproximar el problema a variables \emph{i.i.d}, que habilitan el uso de los métodos de aprendizaje supervisado del ML.
\end{enumerate}

% Función de recompensa según tipos de tareas dependiendo periódicas o discretas, forma parametrizable y basada en Bolza, codificación de la tarea. Representación de la intención del agente
% La señal de recompensa es una representación compacta de la intención del humanoide y contiene información que lleva al agente hacia sus objetivos. Ese conocimiento suele ser transferible. Generalmente es dada por el programador, el cual tiene experiencia en el problema o es derivada de la experiencia del agente. Recompensa descontada, tiempo finito o recompensa acumulada promedio. Función no-lineal de recompensa, Bayesian IRL, Maximum entropy IRL y PI$^2$. El cuello de botella. Expertos con incompletas e imperfectas demostraciones. Una función de recompensa sirve para representaciones portables de comportamientos, permitiendo la generalización a nuevos dominios. Usando DNN se pierde la estructura diseñada a mano por los algoritmos IRL tradicionales
La función de costo o recompensa inicialmente se probará como una señal dispersa sobre una estructura de Bolza tradicional, no obstante por su lenta convergencia, es necesario usar el \emph{Aprendizaje por Refuerzo Inverso} (IRL) o \emph{Control Óptimo Inverso} (IOC), el cual encuentra la función de recompensa, a partir de ejemplos demostrativos que son asumidos frecuentemente como óptimos. En este caso, se propone que los ejemplos serán sub-óptimos tomados de una muestra de trayectorias, de las cuales se asumirá un modelo probabilístico que debe ser estimado. 

En la señal de recompensa se codifican los comportamientos de forma natural con la interacción del entorno y a diferencia de buscar solo una política, la principal ventaja de su extracción es, que las funciones de costo son transferibles a distintas morfologías. Sin embargo existe el inconveniente de que muchas funciones de costo representan el mismo comportamiento, con lo cual la solución del problema de IRL es definido como \emph{mal condicionado}. Para enfrentar este \emph{ill-posed}, se usarán algoritmos basados en estimaciones probabilísticas, en principio, la \emph{Máxima Entropía}. 

Otra ventaja es que dependiendo de la señal de recompensa, el aprendizaje de una tarea puede ser notoriamente acelerado. Para aprender una buena señal de costo se utilizará: (1) en la \emph{representación de la función de recompensa}, (i) combinaciones lineales de funciones características dependientes únicamente del estado, diseñadas previamente basadas en el conocimiento de la locomoción, como por ejemplo el ZMP o el IPM. Y también, (ii) redes neuronales profundas (DNN), las cuales son una representación no lineal de las características e inclusive una representación directa de la señal sensoriomotora, sobre la cual se puede extraer características sin tener que diseñarlas previamente; (2) los \emph{algoritmos de IRL a explorar} de: (i) ${PI}^2$ basado en DMP para el problema inverso~\cite{Kalakrishnan2010}, (ii) el REPS para IRL~\cite{Boularias2011} y (iii) \emph{Guided Cost Learning} basado en una estructura de costo DNN y Máxima Entropía, capaz de enfrentar problemas de alta dimensión de modelos dinámicos desconocidos con un número bajo de ejemplos~\cite{Finn2016}. 

%Para la búsqueda por RL, la función de costo o recompensa, se propondrá de diferentes formas: (1) manual de forma \emph{dispersa}, (2) mediante \emph{aprendizaje por refuerzo inverso} (IRL) del ejemplo sub-óptimo demostrativo de locomoción, (3) mediante IRL de la caminata del modelo NMS de Geyer.

%parte de la metodología
La combinación de aprendizaje por imitación y RL, denominado \emph{aprendizaje de principiante}\footnote{Apprenticeship Learning}, se caracteriza por la necesidad de un instructor y la práctica realizada por el aprendiz. Es posible a partir de las demostraciones encontrar óptimos locales, que nos permitan tomar las demostraciones como políticas iniciales de partida. Muchas veces las demostraciones humanas a humanoides deben ser adaptadas teniendo en cuenta las diferencias morfológicas entre el instructor y el robot. Otras veces el instructor controla directamente el robot, para poder crear una demostración válida. En este caso las demostraciones partirán de controles humanoides reconocidos y comportamientos auto-organizados, que no necesariamente se basan sobre modelos NMS pero que si tienen una estructura humanoide, presentes en trabajos anteriores de humanoides. Mediante este esquema de aprendizaje, las LSTM se explorarán como mecanismo de \emph{repetir para recordar y recordar para repetir} y como mecanismo para enfrentar los POMDPs en el sistema NMS.

La estructura esquemática del entorno-cerebro y el lazo sensoriomotor está representada en la figura \ref{fig:ObjGen}, en donde se exhibe, cómo el lazo sensoriomotor se acopla al cerebro por un lado y por el otro al entorno. El entorno incluye el cuerpo del humanoide. El lazo sensoriomotor es el sistema periférico representado por parte del sistema NMS, específicamente la parte que controla al sistema músculo-esquelético por medio de la dinámica neuronal de la activación del modelo muscular de Hill, los reflejos y los modelos de CPGs. Fisiológicamente desde las señales eferentes-aferentes, la médula y hasta antes del \emph{tronco encefálico}\footnote{brainstem}. Se puede apreciar, que tiene diferentes mecanismos de adaptación: (1) aprendizaje auto-organizado, (2) aprendizaje supervisado y (3) aprendizaje por refuerzo. El aprendizaje de principiante se realiza utilizando una combinación de los tres mecanismos anteriores.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\textwidth]{images/ObjetivoGeneral}
  \caption[Estructura general de emergencia y aprendizaje]{Estructura general de emergencia y aprendizaje de locomoci\'on basada en cognici\'on y reflejos}
  \label{fig:ObjGen}
\end{figure}

\section{Objetivo 1: Construcción del modelo humanoide}
\label{sec:obj1}
% Caso 1. Locomoción de marcha Extrínsicamente motivado
% Caso 2. Balance Intrínsicamente motivado (autonomía, sorpresa, curiosidad, innovación)\cite{Kappen2007}
Dos casos de estudio: (1) el primero la caminata en el plano con control de dirección para cumplir objetivos de posición, partiendo de ejemplos demostrativos; y (2) el otro consiste en comportamientos auto-organizado de mantener el equilibrio mediante la colocación del pie y la utilización de los brazos, en un ambiente pre-estructurado. Con los casos de estudio anteriores mediante \emph{aprendizaje de principiante} se forma un conjunto de datos sensoriomotores que permitirá el desarrollo del comportamiento y el control del humanoide, mejorando su desempeño a través del ensayo y error.

% Proponer una estrategia que evolucione los controladores de locomoción humanoide actuales basados en ZMP hacia una locomoción más natural
Mediante el primer caso de estudio, se utiliza una caminata basada en ZMP y HZD similar a la de~\cite{Wang2012}, para tomar la estructura de control y locomoción propuestas, como punto de partida de un ejemplo sub-óptimo demostrativo de locomoción. Esto con el fin de que el sistema NMS aprenda dicho comportamiento y a partir de ello, busque mediante RL una solución óptima local, que mantenga el balance dinámico pero que elimine la restricción del ZMP, elimine la flexión de rodilla y permita la rotación del pie, en busca de una locomoción más natural y eficiente. En este paso la optimización que requiere las \emph{restricciones virtuales}, se hará mediante DMPs como un caso especial del los MO. Otra ventaja de usar las restricciones virtuales es que a través de ellas se puede probar otros controladores clásico basados en IPM, LIPM, SLIP, los cuales representan características del sistema sensorial utilizados para el diseño de trayectorias articulares de la caminata. El SLIP es un \emph{template} muy importante para el estudio de la locomoción con extremidades y es un punto de partida para estructurar los ensayos mentales, el modelo de D-SLIP basado en HZD~\cite{Kobayashi2016}, se tomará como modelo inicial interno predictivo para ser representado mediante una estructura neuronal en el segundo objetivo.

Mediante otro caso de estudio, basado en los trabajos de \emph{Playful Machines} de~\cite{Der2012,Martius2013,Der2015}, en donde se explora la formación de objetivos sobre la excitación del lazo sensoriomotor, para producir emergencia de comportamientos que son auto-organizados en el sistema NMS, se analizará el caso de ponerse en pie y mantener el equilibrio, ya sea con ayuda de los brazos o mediante la colocación del pie. La maximización de la información predictiva será la clave en la exploración de estos comportamientos. En este paso, tal y como se obtuvo en~\cite{Martius2014}, a través de ciertos patrones deseados de señal sensorial, se buscará los movimientos deseados. Aquí el ejercicio de transferir el comportamiento al sistema NMS propuesto, es por medio de una estructura auto-organizable presentada sobre el sistema NMS de~\cite{Marques2014}. El objetivo principal acá es definir un conjunto de sensores y patrones sensoriales que permitan construir características y funciones de costo, con el fin de poder transferir el comportamiento a diferentes morfologías de forma natural mediante la percepción dichos sensores y patrones. Dichas transferencias serán analizadas en el tercer objetivo.

\section{Objetivo 2: Modelos internos del lazo sensoriomotor}
\label{sec:obj2}
Por modelos internos se entenderá como: (1) representaciones codificadas del comportamiento del humanoide a través de la función de costo; (2) modelos dinámicos, cinemáticos y sensoriomotores de la estructura NMS del modelo humanoide; y (3) modelos predictivos basados en los dos anteriores, utilizando modelos gráficos que permitan detectar fallos y corregir o abortar las estrategias de control en ejecución, representando una herramienta importante para la toma de decisiones; Los dos últimos tipos de modelos se utilizarán como \emph{ensayos mentales}, para aumentar la eficiencia estadística del aprendizaje del bípedo, mediante aprendizaje del modelo.

Para los dos casos de estudio del primer objetivo, el diseño de la señal de recompensa se implementará de forma jerárquica y modular utilizando las técnicas de IRL descritas anteriormente en la sección de búsqueda de costos. Para explorar los mecanismos de corto y largo plazo presentes en el sistema NMS y su relación con la estructura de controladores jerárquicos, se utiliza el segundo caso de estudio del objetivo anterior. Utilizando el control básico de equilibrio auto-organizado, que se obtuvo del análisis del caso, donde la postura (incluyendo la ayuda de las extremidades superiores) y colocación del pie, se controlan inicialmente por una capa de bajo nivel constituida por reflejos. En este segundo objetivo se construye un controlador jerárquico con dos capas adicionales. La primera, una capa intermedia de la colocación del pie y el balance donde se puede controlar a voluntad, mediante un proceso no reflexivo que implica un mayor tiempo de cómputo. Y en una última capa, la estrategia de desplazamiento general en el entorno es construida, con la finalidad de alcanzar un lenguaje de comunicación entre el entorno y robot, mediante pequeñas misiones de desplazamiento y paso de obstáculos, al modular la señal de recompensa por medio de sus características. Las capas intermedia y superior deben estar dotadas de modelos internos del cerebro-entorno.

Para la implementación de los \emph{ensayos mentales}, el lazo sensoriomotor será representado por un modelo gráfico propuesto por~\cite{Ay2015}, con diferentes tipos de arquitecturas del Deep Learning. Algunas de ellas, de interés en esta investigación serán evaluadas: Deep NNs, LSTM, Deep RBM, Deep Belief Networks (DBN), para aproximar modelos de distribuciones presentes en un modelo bayesiano, como se especula en~\cite{Goodfellow2016}. Una ventaja de estos modelos es que se puede ajustar el tamaño de la red a mediada que la complejidad del comportamiento aumenta como es explicado en~\cite{Montufar2015}. La integración de los comportamientos aprendidos en el primer objetivo, implementados sobre un único controlador, proponiendo un control jerárquico, que module las tareas dependiendo de una comunicación básica entre el entorno y el controlador, es evaluada acá. Adicional a esto, pruebas sobre entornos ligeramente no-estructurados y perturbaciones externas son probados también sobre el controlador propuesto en este objetivo. 

\section{Objetivo 3: Metodología de transferencia del controlador}
\label{sec:obj3}
En este objetivo, se desea analizar las estructuras de los modelos internos que constituyen los controladores del segundo objetivo y comprarlos con los originales de ZMP-HZD, para proponer criterios de control más parecidos a los de la locomoción humana. Aquí basados en la hipótesis de que la función de costo codifica el comportamiento sin importar diferencias morfológicas siempre y cuando existan unas referencias de patrones sensoriales, se plantea probar el controlador cambiando la morfología humanoide con asimetrías corporales y lesiones articulares, esperando obtener un control autónomo que se configure a sí mismo ante un cambio de morfología. Por lo tanto es necesario definir unas reglas de transferencia sobre el sistema sensoriomotor del control \emph{transferente} al \emph{transferido}, sobre los cuales se asegure una similitud mínima. 

Aprovechando la riqueza del lazo sensoriomotor del sistema NMS, donde se tiene señales de posición, velocidad, fuerza y otras propiocepciones más, se explora la transferencia de un control con sistema NMS: (1) a un control mediante restricciones virtuales y HZD; y (2) a un control por impedancia; Los resultados de este controlador se probarán sobre un robot humanoide de bajo costo como el Bioloid acondicionado según requerimientos. Sobre ese robot, aprovechando su diseño modular se planea realizar pruebas sobre configuraciones asimétricas.

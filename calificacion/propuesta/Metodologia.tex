\chapter{Metodología}
\label{chap:metho}
Para lograr el objetivo general, en la metodología se propone utilizar el modelo NMS de Geyer~\cite{Geyer2010} extendido en~\cite{Song2015,Dzeladini2014}, como representación de la planta real del humanoide a estudiar. Existe varias ventajas de tomar ese modelo: (1) Desde el punto de vista de la computación morfológica, la complejidad de la señal de control, como se demostró~\cite{Ghazi-Zahedi2015a}, permite simplificar el espacio de estado-acción, imponiendo además restricciones naturales presentes en el cuerpo humano y explotando la riqueza dinámica natural de la auto-estabilización; (2) La implementación de diferentes tipos de retardos sobre diferentes \emph{pathways}, permite formar acciones de control reactivas y otras predictivas, estas últimas requieren de un computo mayor necesario para tomar una decisión más elaborada; (3) Aprovechando el modelo extendido de~\cite{Song2015}, la estructura jerárquica permite diferentes tipos de movimientos como caminar, correr y transiciones en diferentes tipos de terrenos, los cuales son ejemplos demostrativos de movimiento útiles para el aprendizaje; (4) Aprovechando el modelo extendido de~\cite{Dzeladini2014} de la capa de CPGs, se permite modular los reflejos y formar predicciones de la interacción, además guardar dichas configuraciones de los CPGs como modelos predictivos, para practicar el \emph{ensayo mental}. 

La implementación del modelo de Geyer, propuesto en~\cite{Dzeladini2014}, tiene una librería del sistema NMS desarrollada en C++ y es fácil acoplar a un motor físico de dinámica multicuerpo. En este caso para el motor físico, las librerías RDBL desarrolladas por Martis Felis, de los algoritmos de~\cite{Featherstone2008} que incluyen un modelo de contacto con fricción, desarrolladas en C++ y con extensión a Python, son escogidas por la versatilidad del código y su extensión para la optimización con GPU mediante CUDA.

% Proponer una estrategia que evolucione los controladores de locomoción humanoide actuales basados en ZMP hacia una locomoción más natural
Mediante un caso de estudio, se utiliza una caminata basada en ZMP y HZD similar a la de~\cite{Wang2012}, para tomar la estructura de control y locomoción propuestas, como punto de partida de un ejemplo sub-óptimo demostrativo de locomoción, con el fin de que el sistema NMS aprenda dicho comportamiento y a partir de esto, busque mediante RL una solución óptima local, que mantenga el balance dinámico pero que elimine la restricción del ZMP, elimine la flexión de rodilla y permita la rotación del pie, en busca de una locomoción más natural.

Para la búsqueda por RL, la función de costo o recompensa, se propondrá de diferentes forma: (1) manual de forma \emph{dispersa}, (2) mediante IRL del ejemplo sub-óptimo demostrativo de locomoción, (3) mediante IRL de la caminata del modelo NMS de Geyer.

Para la búsqueda de la estrategia mediante RL Robótico, configurar el algoritmo adecuado de RRL, requiere de pruebas de diferentes elecciones, que al final por el problema específico del humanoide con NMS, se sabrá cual combinación es más adecuada. Por lo tanto especificar la configuración del algoritmo de RRL es un resultado que debe ser encontrado. La representación de la política será mediante redes neuronales (ANNs) y Osciladores morfados (MOs), estos últimos~\cite{Ajallooeian2013} son una generalización de los DMPs y los CPGs. En un problema RRL es posible configurar diferentes estrategias~\cite{Kober2013,Deisenroth2013}, tres grandes ramas serán claramente estudiadas: (1) Por \emph{evaluación de la política}, donde la evaluación puede ser cada paso o cada episodio; (2) Por \emph{actualización de la política} en donde se debe analizar entre algoritmos basados en el \emph{gradiente de la política}, el algoritmo EM, las métricas de Información (IT), las integrales de camino ($PI^2$) y la optimización estocástica; y la última (3) Por \emph{estrategia de exploración}, que tiene aún más posibilidades, las cuales pueden funcionar en conjunto. Estas son: \emph{espacio de la perturbación} sobre el espacio de acción o el espacio de parámetros, en el último se puede estructurar el ruido y trabajar un nivel superior de aprendizaje de politica; \emph{escala de tiempo} nuevamente esta por paso o por episodio, en el primero se puede evitar los mínimos locales; \emph{distribución de ruido} se usa matrices de correlación basadas en métodos de segundo orden, aca el método CMA-ES será estudiado por sus resultados; y \emph{actualización de la distribución del ruido} donde se puede variar las tasas de exploración y utilizar métricas de entropía relativa.

En cuanto al problema de volver el RRL tratable, los retos en la metodología son: 
\begin{enumerate}
\item El \emph{problema de la dimensionalidad}  Utilizando \emph{Osciladores Morfados} (MO) en el nivel de activación muscular; las sinergias musculares propuestas y la capacidad de cómputo del MC en el modelo NMS presentado en~\cite{Song2015} para la reducción; y la búsqueda local partiendo de ejemplos demostrativos. 
\item La \emph{exploración cuidadosa}, la perturbación será basada en gradiente sobre el espacio de acciones o sobre el espacio de los parámetros que aproximan la política, la exploración en el tiempo debe ser estudiada en el paso y en el episodio.
\item Le \emph{eficiencia estadística}
\end{enumerate}

Mediante otro caso de estudio, basado en el trabajos de \emph{Playful Machines} de~\cite{Der2012,Martius2013,Der2015}, en donde se explora la formación de objetivos sobre la excitación del lazo sensoriomotor, para producir emergencia de comportamientos que son auto-organizados en el sistema NMS, se analiza el caso de ponerse en pie y mantener el equilibrio ya sea con ayuda de los brazos o mediante la colocación del pie. La maximización de la información predictiva será la clave en la exploración de comportamientos, en este paso, tal y como se obtuvo en~\cite{Martius2014}. Aquí el ejercicio de transferir el comportamiento al sistema NMS es por medio de una estructura de auto-organizable propuesta sobre el sistema NMS de~\cite{Marques2014}, será utilizada como representación morfológica.

%parte de la metodología
La combinación de aprendizaje por imitación y RL, denominado \emph{aprendizaje de principiante}\footnote{Apprenticeship Learning}, se caracteriza por la necesidad de un profesor y la practica realizada por el aprendiz. Es posible a partir de las demostraciones encontrar óptimos locales que nos permitan tomar las demostraciones como políticas iniciales de partida. Muchas veces las demostraciones humanas a humanoides deben ser adaptadas teniendo en cuenta las diferencias cinemáticas entre el profesor y el robot. Otras veces el profesor controla directamente el robot  poder obtener una demostración.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\textwidth]{images/ObjetivoGeneral.png}
  \caption{Estructura general de emergencia y aprendizaje de locomoci\'on basada en cognici\'on y reflejos}
  \label{fig:ObjGen}
\end{figure}



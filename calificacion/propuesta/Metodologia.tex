\chapter{Metodología}
\label{chap:metho}

\section{La maquinaria por debajo}
\label{sec:maquinaria}
Para lograr el objetivo general, en la metodología se propone utilizar el modelo NMS de Geyer~\cite{Geyer2010} extendido en~\cite{Song2015,Dzeladini2014}, como representación de la planta real del humanoide a estudiar. Existe varias ventajas de tomar ese modelo: (1) Desde el punto de vista de la computación morfológica, la complejidad de la señal de control, como se demostró en~\cite{Ghazi-Zahedi2015a}, permite simplificar el espacio de estado-acción, imponiendo además restricciones naturales presentes en el cuerpo humano y explotando la riqueza dinámica natural de la auto-estabilización de algunos comportamientos mediante la \emph{mórfosis} del sistema; (2) La implementación de diferentes tipos de retardos sobre diferentes \emph{pathways}, permite formar acciones de control reactivas y otras de control predictivas, estas últimas requieren de un mayor cómputo y un mayor tiempo, que son necesarios para tomar una decisión más elaborada. Esta característica constituye el fundamento morfológico de un control jerárquico; (3) Aprovechando el modelo extendido de~\cite{Song2015}, la estructura jerárquica permite diferentes tipos de movimientos como caminar, correr y transiciones de velocidades. Los tres  en diferentes tipos de terrenos. Estos son ejemplos demostrativos de movimiento, útiles para el aprendizaje; (4) Aprovechando el modelo extendido de~\cite{Dzeladini2014} de la capa de CPGs, se permite modular los reflejos y formar predicciones de la interacción, además de guardar dichas configuraciones de los CPGs como modelos predictivos, esto es importante para practicar el \emph{ensayo mental}\footnote{mental rehearsal}. 

La implementación del modelo de Geyer, propuesto en~\cite{Dzeladini2014}, tiene una librería del sistema NMS desarrollada en C++ y es fácil acoplar a un motor físico de dinámica multicuerpo. En este caso para el motor físico, las librerías RDBL desarrolladas por Martis Felis, de los algoritmos de~\cite{Featherstone2008} que incluyen un modelo de contacto con fricción, desarrolladas en C++ y con extensión a Python, son escogidas por la versatilidad del código y su extensión para la optimización con GPU mediante CUDA.

% modelo de friccion, por que python?, por que C++, por que CUDA? visualizacion como post-proceso

%Probability is about modeling uncertainty in the world.
%Inference is about learning properties of the world from data that we observe.
%Computation is needed to automate probabilistic analysis and inference, especially to handle massive data sets.
%Sterotypical movements enable prediction and statistics.
%For every movement skill, we associate motor and perceptual variables and theit statistics over time.
%This information, represented in memory as ASM, can be used in error corretion, pattern completion, switching and abort behaviors 

\section{La piedra angular de adaptación: Búsqueda de políticas}
\label{sec:piedra}
Para la búsqueda de la políticas mediante RL Robótico, configurar el algoritmo adecuado de RRL, requiere de pruebas con diferentes elecciones. Por el problema específico del humanoide acoplado a un sistema NMS, solo al final de las pruebas se sabrá cuál combinación es la más adecuada. Por lo tanto especificar la configuración del algoritmo de RRL, es un resultado que debe ser encontrado. La representación de la política será mediante redes neuronales (ANNs), primitivas de movimiento dinámicas (DMPs)\footnote{Las PMP también son una opción que se estudiará} y Osciladores morfados (MOs), estos últimos~\cite{Ajallooeian2013} son una generalización de los DMPs rítmicos y los CPGs. Un algoritmo RRL tiene por lo general tres ingredientes: un paso de evaluación de políticas, otro paso de mejoramiento o actualización de políticas y un método de exploración. Es posible configurar dichos ingredientes utilizando diferentes estrategias~\cite{Kober2013,Deisenroth2013}. Los tres ingredientes serán claramente estudiados, teniendo en cuenta: (1) Por \emph{evaluación de la política}, donde la evaluación puede ser cada paso o cada episodio, o donde se puede utilizar las técnicas de \emph{diferencias temporales} (TD); (2) Por \emph{actualización de la política}, en donde se debe elegir entre algoritmos basados en: el \emph{gradiente de la política}, el algoritmo EM\footnote{Expectación-Maximización}, las métricas de Información (IT), las integrales de camino ($PI^2$) y la optimización estocástica; y la última (3) Por \emph{estrategia de exploración}, que tiene aún más posibilidades, las cuales pueden funcionar en conjunto, dando múltiples combinaciones posibles. Estas son: (i) \emph{espacio de la perturbación}, sobre el espacio de acción o el espacio de parámetros, en el último se puede estructurar el ruido y trabajar un nivel superior de aprendizaje de política; (ii) \emph{escala de tiempo}, nuevamente esta por paso o por episodio, en el primero se puede evitar los mínimos locales y en el segundo, las actualizaciones de las políticas resultan más confiables; (iii) \emph{distribución de ruido}, en donde se usa matrices de correlación basadas en métodos de segundo orden, acá el método de \emph{Covariance Matrix Adaptation} (CMA) será estudiado por sus resultados; y (iv) \emph{actualización de la distribución del ruido} donde se puede variar las tasas de exploración y utilizar métricas de entropía relativa a medida que itera el algoritmo.

En cuanto al problema de volver el RRL tratable, los retos en la metodología son: 
\begin{enumerate}
\item El \emph{problema de la dimensionalidad} que se enfrentará mediante tres ideas: (1) la representación de políticas parametrizadas y estructuradas, utilizando MOs para movimientos rítmicos y DMPs para movimientos discretos, aplicados al nivel de activación muscular como representación de las políticas; (2) las sinergias musculares propuestas y la capacidad de cómputo del MC en el modelo NMS presentado en~\cite{Song2015} como la reducción de la dimensionalidad, la simplificación de la señal de control y el manejo de variables latentes; y (3) la búsqueda local partiendo de ejemplos demostrativos. 
\item La \emph{exploración cuidadosa}, mediante las cuatro opciones de estrategias de exploración descritas anteriormente, busca políticas que no sean nocivas o destructivas para la estructura del humanoide. El punto clave aquí, es una exploración local que también contribuye para tratar el problema de la dimensionalidad.
\item La \emph{eficiencia estadística}. En este punto el manejo eficiente de grandes cantidades de datos, procedentes de la información del lazo sensoriomotor al interactuar, se logra de la formación de modelos probabilísticos internos, que representan modelos directos locales, estimadores de estado, modelos de recompensa, predicciones de fallos ante perturbaciones y otros más, que permiten la capacidad cognitiva de realizar ensayos metales produciendo un manejo eficiente de los datos y una eficiencia estadística que reduce el número de muestras. El uso de \emph{mini-batches} y un manejo adecuado de los datos de muestra, permite aproximar el problema a variables \emph{i.i.d}, que habilitan el uso de los métodos de aprendizaje supervisado del ML.
\end{enumerate}

% Función de recompensa según tipos de tareas dependiendo periódicas o discretas, forma parametrizable y basada en Bolza
Para la búsqueda por RL, la función de costo o recompensa, se propondrá de diferentes formas: (1) manual de forma \emph{dispersa}, (2) mediante \emph{aprendizaje por refuerzo inverso} (IRL) del ejemplo sub-óptimo demostrativo de locomoción, (3) mediante IRL de la caminata del modelo NMS de Geyer.

%parte de la metodología
La combinación de aprendizaje por imitación y RL, denominado \emph{aprendizaje de principiante}\footnote{Apprenticeship Learning}, se caracteriza por la necesidad de un instructor y la practica realizada por el aprendiz. Es posible a partir de las demostraciones encontrar óptimos locales, que nos permitan tomar las demostraciones como políticas iniciales de partida. Muchas veces las demostraciones humanas a humanoides deben ser adaptadas teniendo en cuenta las diferencias morfológicas entre el instructor y el robot. Otras veces el instructor controla directamente el robot, para poder crear una demostración válida. En este caso las demostraciones partirán de controles humanoides reconocidos y comportamientos auto-organizados, que no necesariamente se basan sobre modelos NMS pero que si tienen una estructura humanoide, de trabajos anteriores.

La estructura esquemática del entorno-cerebro y el lazo sensoriomotor está representada en la figura \ref{fig:ObjGen}, en donde se exhibe, cómo el lazo sensoriomotor se acopla al cerebro por un lado y por el otro al entorno. El entorno incluye el cuerpo del humanoide. El lazo sensoriomotor es el sistema periférico representado por parte del sistema NMS, específicamente la parte que controla al sistema músculo-esquelética por medio de la dinámica neuronal de la activación del modelo muscular de Hill, los reflejos y los modelos de CPGs. Fisiológicamente desde las señales eferentes-aferentes, la médula y hasta antes del \emph{tronco encefálico}\footnote{brainstem}. Se puede apreciar, que tiene diferentes mecanismos de adaptación: (1) aprendizaje auto-organizado, (2) aprendizaje supervisado y (3) aprendizaje por refuerzo. El aprendizaje de principiante se realiza utilizando una combinación de los tres mecanismos anteriores.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\textwidth]{images/ObjetivoGeneral}
  \caption[Estructura general de emergencia y aprendizaje]{Estructura general de emergencia y aprendizaje de locomoci\'on basada en cognici\'on y reflejos}
  \label{fig:ObjGen}
\end{figure}

\section{Objetivo 1: Construcción del modelo humanoide}
\label{sec:obj1}
Dos casos de estudio: (1) el primero la caminata en el plano con control de dirección para cumplir objetivos de posición, partiendo de ejemplos demostrativos; y (2) el otro consiste en comportamientos auto-organizado de mantener el equilibrio mediante la colocación del pie y la utilización de los brazos, en un ambiente pre-estructurado. Con los casos de estudio anteriores mediante \emph{aprendizaje de principiante} se forma un conjunto de datos que permitirá el desarrollo del comportamiento y el control del humanoide, mejorando su desempeño a través del ensayo y error.

% Proponer una estrategia que evolucione los controladores de locomoción humanoide actuales basados en ZMP hacia una locomoción más natural
Mediante el primer caso de estudio, se utiliza una caminata basada en ZMP y HZD similar a la de~\cite{Wang2012}. Para tomar la estructura de control y locomoción propuestas, como punto de partida de un ejemplo sub-óptimo demostrativo de locomoción. Esto con el fin de que el sistema NMS aprenda dicho comportamiento y a partir de ello, busque mediante RL una solución óptima local, que mantenga el balance dinámico pero que elimine la restricción del ZMP, elimine la flexión de rodilla y permita la rotación del pie, en busca de una locomoción más natural. En este paso la optimización que requiere las \emph{restricciones virtuales}, se hará mediante DMPs como un caso especial del los MO. Otra ventaja de usar las restricciones virtuales es que a través de ellas se puede probar otros controladores clásico basados en IPM, LIPM, SLIP. El ultimo constituye un \emph{template} muy importante para el estudio de la locomoción con extremidades y es un punto de partida para los ensayos mentales.

Mediante otro caso de estudio, basado en el trabajos de \emph{Playful Machines} de~\cite{Der2012,Martius2013,Der2015}, en donde se explora la formación de objetivos sobre la excitación del lazo sensoriomotor, para producir emergencia de comportamientos que son auto-organizados en el sistema NMS, se analizará el caso de ponerse en pie y mantener el equilibrio, ya sea con ayuda de los brazos o mediante la colocación del pie. La maximización de la información predictiva será la clave en la exploración de estos comportamientos. En este paso, tal y como se obtuvo en~\cite{Martius2014}, a través de ciertos patrones deseados de señal sensorial, se buscará los movimientos deseados. Aquí el ejercicio de transferir el comportamiento al sistema NMS propuesto, es por medio de una estructura auto-organizable presentada sobre el sistema NMS de~\cite{Marques2014}.

\section{Objetivo 2: Modelos internos del lazo sensoriomotor}
\label{sec:obj2}
Para explorar los mecanismos de corto y largo plazo presentes en el sistema NMS y su relación con la estructura de controladores jerárquicos, se utiliza el segundo caso de estudio del objetivo anterior. Utilizando el control básico de equilibrio auto-organizado, que se obtuvo del análisis del caso, donde la postura (incluyendo la ayuda de las extremidades superiores) y colocación del pie, se controlan inicialmente por una capa de bajo nivel constituida por reflejos. En este segundo objetivo se construye un controlador jerárquico con dos capas adicionales. La primera, una capa intermedia de la colocación del pie y el balance donde se puede controlar a voluntad, mediante un proceso no reflexivo que implica un mayor tiempo de cómputo. Y en una última capa, la estrategia de desplazamiento general en el entorno es construida, con la finalidad de alcanzar un lenguaje de comunicación entre el entorno y robot, mediante pequeñas misiones de desplazamiento y paso de obstáculos. El diseño de la señal de recompensa de implementará de forma jerárquica y modular utilizando las técnicas de IRL. Las capas intermedia y superior deben estar dotadas de modelos internos del cerebro-entorno.

Para la implementación de los \emph{ensayos mentales}, el lazo sensoriomotor será representado por un modelo gráfico propuesto por~\cite{Ay2015}, con diferentes tipos de arquitecturas del Deep Learning. Algunas de ellas, de interés en esta investigación serán evaluadas: Deep NNs, LSTM, Deep RBM, Deep Belief Networks (DBN), para aproximar modelos de distribuciones presentes en un modelo bayesiano, como se especula en~\cite{Goodfellow2016}. Una ventaja de estos modelos es que se puede ajustar el tamaño de la red a mediada que el comportamiento aumenta como es explicado en~\cite{Montufar2015}. La integración de los comportamientos aprendidos en el primer objetivo, implementados sobre un único controlador, proponiendo un control jerárquico, que module las tareas dependiendo de una comunicación básica entre el entorno y el controlador. 

\section{Objetivo 3: Metodología de transferencia del controlador}
\label{sec:obj3}
En el último de los objetivos, se desea analizar las estructuras de los modelos internos que constituyen los controladores de los objetivos anteriores y comprarlos con los originales de ZMP-HZD para proponer criterios de control más parecidos a los de la locomoción humana. La transferencia a un humanoide con NMS virtual mediante restricciones virtuales y HZD, es el paso final para cerrar.

\chapter{Metodología}
\label{chap:metho}

\section{La maquinaria por debajo}
\label{sec:maquinaria}
Para lograr el objetivo general, en la metodología se propone utilizar el modelo NMS de Geyer~\cite{Geyer2010} extendido en~\cite{Song2015,Dzeladini2014}, como representación de la planta real del humanoide a estudiar. Existe varias ventajas de tomar ese modelo: (1) Desde el punto de vista de la computación morfológica, la complejidad de la señal de control, como se demostró en~\cite{Ghazi-Zahedi2015a}, permite simplificar el espacio de estado-acción, imponiendo además restricciones naturales presentes en el cuerpo humano y explotando la riqueza dinámica natural de la auto-estabilización; (2) La implementación de diferentes tipos de retardos sobre diferentes \emph{pathways}, permite formar acciones de control reactivas y otras de control predictivas, estas últimas requieren de un mayor cómputo y un mayor tiempo, que necesarios para tomar una decisión más elaborada. Esta característica constituye el fundamento morfológico de un control jerárquico; (3) Aprovechando el modelo extendido de~\cite{Song2015}, la estructura jerárquica permite diferentes tipos de movimientos como caminar, correr y transiciones de velocidades en diferentes tipos de terrenos. Estos son ejemplos demostrativos de movimiento, útiles para el aprendizaje; (4) Aprovechando el modelo extendido de~\cite{Dzeladini2014} de la capa de CPGs, se permite modular los reflejos y formar predicciones de la interacción, además guardar dichas configuraciones de los CPGs como modelos predictivos, esto es importante para practicar el \emph{ensayo mental}. 

La implementación del modelo de Geyer, propuesto en~\cite{Dzeladini2014}, tiene una librería del sistema NMS desarrollada en C++ y es fácil acoplar a un motor físico de dinámica multicuerpo. En este caso para el motor físico, las librerías RDBL desarrolladas por Martis Felis, de los algoritmos de~\cite{Featherstone2008} que incluyen un modelo de contacto con fricción, desarrolladas en C++ y con extensión a Python, son escogidas por la versatilidad del código y su extensión para la optimización con GPU mediante CUDA.

\section{La piedra angular de adaptación: Búsqueda de estrategias}
\label{sec:piedra}
Para la búsqueda de la estrategia mediante RL Robótico, configurar el algoritmo adecuado de RRL, requiere de pruebas con diferentes elecciones, que al final por el problema específico del humanoide con NMS, se sabrá por medio de ensayos cuál combinación es más adecuada. Por lo tanto especificar la configuración del algoritmo de RRL, es un resultado que debe ser encontrado. La representación de la política será mediante redes neuronales (ANNs) y Osciladores morfados (MOs), estos últimos~\cite{Ajallooeian2013} son una generalización de los DMPs rítmicos y los CPGs. En un problema RRL es posible configurar diferentes estrategias~\cite{Kober2013,Deisenroth2013}, tres grandes ramas serán claramente estudiadas: (1) Por \emph{evaluación de la política}, donde la evaluación puede ser cada paso o cada episodio; (2) Por \emph{actualización de la política}, en donde se debe elegir entre algoritmos basados: en el \emph{gradiente de la política}, el algoritmo EM, las métricas de Información (IT), las integrales de camino ($PI^2$) y la optimización estocástica; y la última (3) Por \emph{estrategia de exploración}, que tiene aún más posibilidades, las cuales pueden funcionar en conjunto, dando múltiples combinaciones posibles. Estas son: \emph{espacio de la perturbación}, sobre el espacio de acción o el espacio de parámetros, en el último se puede estructurar el ruido y trabajar un nivel superior de aprendizaje de política; \emph{escala de tiempo} nuevamente esta por paso o por episodio, en el primero se puede evitar los mínimos locales; \emph{distribución de ruido} se usa matrices de correlación basadas en métodos de segundo orden, aca el método CMA-ES será estudiado por sus resultados; y \emph{actualización de la distribución del ruido} donde se puede variar las tasas de exploración y utilizar métricas de entropía relativa.

En cuanto al problema de volver el RRL tratable, los retos en la metodología son: 
\begin{enumerate}
\item El \emph{problema de la dimensionalidad} se enfrentará mediante la representación de políticas parametrizadas y estructuradas. Utilizando \emph{Osciladores Morfados} (MO) para movimientos rítmicos y DMPs para movimientos discretos, en el nivel de activación muscular, para representacion de las políticas; las sinergias musculares propuestas y la capacidad de cómputo del MC en el modelo NMS presentado en~\cite{Song2015} para la reducción; y la búsqueda local partiendo de ejemplos demostrativos. 
\item La \emph{exploración cuidadosa}, mediante las opciones de configuración descritas anteriormente, sin embargo el punto clave es una exploración local que también contribuye al problema de la dimensionalidad.
\item La \emph{eficiencia estadística}. En este punto el manejo eficiente de grandes cantidades de datos, procedentes de la información del lazo sensoriomotor al interactuar, se logra de la formación de modelos probabilísticos internos, que representan modelos directos locales, predicciones de fallos ante perturbaciones y otros más, que permiten la capacidad cognitiva de realizar ensayos metales produciendo un manejo eficiente de los datos y una eficiencia estadística que reduce el número de muestras. El uso de \emph{mini-batches} y un manejo adecuado de los datos de muestra, permite aproximar el problema a variables \emph{i.i.d}, que habilitan el uso de los métodos de aprendizaje supervisado del ML.
\end{enumerate}

Para la búsqueda por RL, la función de costo o recompensa, se propondrá de diferentes forma: (1) manual de forma \emph{dispersa}, (2) mediante IRL del ejemplo sub-óptimo demostrativo de locomoción, (3) mediante IRL de la caminata del modelo NMS de Geyer.

%parte de la metodología
La combinación de aprendizaje por imitación y RL, denominado \emph{aprendizaje de principiante}\footnote{Apprenticeship Learning}, se caracteriza por la necesidad de un profesor y la practica realizada por el aprendiz. Es posible a partir de las demostraciones encontrar óptimos locales que nos permitan tomar las demostraciones como políticas iniciales de partida. Muchas veces las demostraciones humanas a humanoides deben ser adaptadas teniendo en cuenta las diferencias cinemáticas entre el profesor y el robot. Otras veces el profesor controla directamente el robot  poder obtener una demostración. En este caso las demostraciones partirán de controles humanoides reconocidos y comportamientos auto-organizados sobre modelos NMS de trabajos anteriores.

La estructura esquemática del entorno-cerebro y el lazo sensoriomotor está representada en la figura \ref{fig:ObjGen}, en donde se exhibe que el lazo sensoriomotor se acopla al cerebro por un lado y por el otro al entorno. El entorno incluye el cuerpo del humanoide. El lazo sensoriomotor es el sistema periférico representado por parte del sistema NMS, específicamente la parte músculo-esquelética y al parte neuronal de la activación del modelo muscular de Hill y modelos de CPGs. Se puede apreciar, que tiene diferentes formas de adaptación (1) aprendizaje auto-organizado, (2) aprendizaje supervisado y (3) aprendizaje por refuerzo. El aprendizaje de principiante se realiza utilizando las tres formas de aprendizaje.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\textwidth]{images/ObjetivoGeneral}
  \caption{Estructura general de emergencia y aprendizaje de locomoci\'on basada en cognici\'on y reflejos}
  \label{fig:ObjGen}
\end{figure}

\section{Primer Objetivo}
\label{sec:obj1}
Dos casos de estudio: (1) el primero la caminata en el plano con control de dirección para cumplir objetivos de posición, partiendo de ejemplos demostrativos; y (2) el otro consiste en comportamientos auto-organizado de mantener el equilibrio mediante la colocación del pie y la utilización de los brazos, en un ambiente pre-estructurado. Con los casos de estudio anteriores mediante \emph{aprendizaje de principiante} se forma un conjunto de datos que permitirá el desarrollo del comportamiento del humanoide, mejorando su desempeño a través del ensayo y error.

% Proponer una estrategia que evolucione los controladores de locomoción humanoide actuales basados en ZMP hacia una locomoción más natural
Mediante el primer caso de estudio, se utiliza una caminata basada en ZMP y HZD similar a la de~\cite{Wang2012}. Para tomar la estructura de control y locomoción propuestas, como punto de partida de un ejemplo sub-óptimo demostrativo de locomoción. Esto con el fin de que el sistema NMS aprenda dicho comportamiento y a partir de ello, busque mediante RL una solución óptima local, que mantenga el balance dinámico pero que elimine la restricción del ZMP, elimine la flexión de rodilla y permita la rotación del pie, en busca de una locomoción más natural. En este paso la optimización que requiere las \emph{restricciones virtuales}, se hará mediante DMPs como un caso especial del los MO.

Mediante otro caso de estudio, basado en el trabajos de \emph{Playful Machines} de~\cite{Der2012,Martius2013,Der2015}, en donde se explora la formación de objetivos sobre la excitación del lazo sensoriomotor, para producir emergencia de comportamientos que son auto-organizados en el sistema NMS, se analiza el caso de ponerse en pie y mantener el equilibrio ya sea con ayuda de los brazos o mediante la colocación del pie. La maximización de la información predictiva será la clave en la exploración de comportamientos, en este paso, tal y como se obtuvo en~\cite{Martius2014}. Aquí el ejercicio de transferir el comportamiento al sistema NMS es por medio de una estructura de auto-organizable propuesta sobre el sistema NMS de~\cite{Marques2014}, será utilizada como representación morfológica.

\section{Segundo Objetivo}
\label{sec:obj2}
Para explorar los mecanismos de corto y largo plazo del sistema NMS para la formación de controles jerárquicos, se puede construir utilizando las habilidades del control básico de equilibrio del comportamiento auto-organizado, obtenido en el segundo caso estudio, donde la postura (incluyendo la ayuda de las extremidades superiores) y colocación del pie se controlan inicialmente por reflejos. En una capa intermedia la colocación del pie y el balance se puede controlar a voluntad mediante un proceso no reflexivo que implica un mayor tiempo de cómputo. En una ultima capa, la estrategia de desplazamiento general del entorno es construida. El diseño de la señal de recompensa de implementará de forma jerárquica y modular.

Para la implementación de los \emph{ensayos mentales}, el lazo sensoriomotor será representado por un modelo gráfico propuesto por~\cite{Ay2015}, con diferentes tipos de arquitecturas del Deep Learning, algunas de ellas son: Deep NNs, LSTM, Deep RBM, Deep Belief Networks (DBN), para aproximar modelos de distribuciones estadísticas como se especula en~\cite{Goodfellow2016}. Una ventaja de estos modelos es que se puede ajustar el tamaño de la red a mediada que el comportamiento aumenta como es explicado en~\cite{Montufar2015}. La integración de los comportamientos aprendidos en el primer objetivo, implementados sobre un único controlador, proponiendo un control jerárquico, que module las tareas dependiendo de una comunicación básica entre el entorno y el controlador. En este punto es importante resaltar que el modelo del NMS no es conocido por los algoritmos de aprendizajes de modelo y se irán aproximando con la práctica.

\section{Tercer Objetivo}
\label{sec:obj3}
En el último de los objetivos, se desea analizar las estructuras de los modelos internos que constituyen los controladores de los objetivos anteriores y comprarlos con los originales de ZMP-HZD para proponer criterios de control más parecidos a los de la locomoción humana.

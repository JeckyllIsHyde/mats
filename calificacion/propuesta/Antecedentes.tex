\chapter{Antecedentes}
\label{chp:ante}

\section{Modelo Neuro-músculo-esquelético}
\label{sec:modNMS}

Tal vez el modelo del sistema neuro-músculo-esquelético NMS que más impacto ha tenido recientemente, propuesto por~\cite{Geyer2010}, es un modelo desarrollado en MATLAB y SimMechanics, proponiendo una locomoción coordinada de la caminata utilizando un conjunto de reflejos, que actúan sobre un conjunto de músculos basados en el modelo de Hill propuesto en~\cite{Geyer2003}, donde se estudia la relación fuerza-longitud-velocidad del músculo y los diferentes circuitos (o \emph{pathways}) que se pueden proponer bajo ese modelo. Los músculos se organizan formando sinergias musculares~\cite{DAvella2003}, de forma que permiten al sistema esquelético: evitar hiperextenciones de las articulaciones, agregar flexibilidad (\emph{Compliance}), evadir el choque con el piso en la etapa de balanceo, reciclar la energía en el talón y otras. Se utiliza algoritmos genéticos (GAs) para la parametrización del sistema NMS.

Posterior a este modelo, Geyer continuó el modelo esta vez para un sistema tridimensional~\cite{Song2015} añadiendo nuevos músculos y reflejos, en donde emergen distintos comportamientos de marcha en modo de: caminar, correr, aceleración, desaceleración, negociación de pendientes y escalones, giro, y evasión deliberada de obstáculos. El controlador es un sistema jerárquico compuesto por dos niveles, en el primero se almacenan y coordinan los reflejos y en el segundo se controla el posicionamiento del pie. El proceso de optimización de los parámetros es cambiado por otra técnica evolutiva recientemente utilizada como método de búsqueda de políticas directa, denominada \emph{Covariance Matrix Adpatation-Evolution Strategy} CMA-ES~\cite{Hansen2006}.

Paralelo al trabajo de~\cite{Geyer2010,Song2015}, André Seyfarth continuo desde~\cite{Geyer2003}, con un trabajo sobre el modelo muscular y su importancia en la comunicación de las señales realimentadas y prealimentadas en la adaptación a un entorno cambiante o con cierto nivel de incertidumbre~\cite{Haeufle2012}. Dentro del canal de comunicación, las señales aferentes provenientes de las fibras sensoriales son estudiadas en detenimiento para el caso de un movimiento periódico de salto. Los husos neuromusculares como receptores sensoriales, llevan la información propioceptiva al controlador, estos son: (1) Longitud y velocidad de alargamiento y contracción del músculo y (2) los \emph{órganos tendinosos de Golgi} llevan la información de la fuerza. Este canal es estudiado concluyendo la importancia de cada una de estas señales en la estabilidad y control del salto, además de especificar si la realimentación es positiva o negativa.

El siguiente paso de Haeufle, al comprobar que las no-linealidades eran útiles en el modelo muscular de~\cite{Haeufle2012}, fue cuantificar la información que fluía por el canal al mirar que tanto computo debía realizar el controlador para lograr una acción específica~\cite{Haeufle2014}, para ello utilizó \emph{la entropía de la información de Shannon}~\cite{MacKay2005}. Ya metido en el campo de la computación morfológica comparó tres modelos de robots saltadores simulados al mirar la complejidad del control en cada uno de los tres siguientes casos: (1) modelo no-lineal del músculo de Hill, (2) modelo lineal del músculo de Hill y (3) motor DC como actuador~\cite{Ghazi-Zahedi2015a}. Otros modelos de músculos basados en Hill son propuestos en~\cite{Schmitt2015} complementan mejor la comparación de un control real.

Extendiendo el trabajo de~\cite{Geyer2010} y paralelo a~\cite{Song2015}, Auke Ijspeert, conocido por su trabajo en CPGs~\cite{Ijspeert2008}, muestra la necesidad de incluir los CPGs en la estructura puramente reflexiva de Geyer en~\cite{Dzeladini2014}. Aunque el modelo de Geyer es estable en presencia de perturbaciones, la modulación de la velocidad y el tamaño del paso no son problemas sencillos y requieren de procesos de optimización \emph{offline} demorados. Al incluir los CPGs en el modelo NMS, la modulación de la velocidad y el paso de la marcha se tornan sencillas y además se muestra que los CPGs pueden ser utilizados como modelos predictivos del lazo sensoriomotor, al ser capaces de reproducir las señales aferentes generadas por una marcha estable. Esta capacidad predictiva de los CPGs ofrece una forma de comparar la importancia relativa de distintos circuitos o pathways de realimentación. Como trabajos futuros, se propone un estudio de co-evolución de las componentes de realimentación y prealimentación. Adicional se concluye que las señales de los pathways pueden ser estudiadas mejor a través de las señales de las motoneuronas, formando señales de baja dimensión compuesta por cuatro primitivas motrices controlando con una mayor facilidad la velocidad, longitud de paso, transición de marchas y la adaptación a pendientes cambiantes. La reducción de dimensionalidad expresada en primitivas de movimiento, de nuevo es estudiada por Ijspeert en~\cite{Sprowitz2014a}, donde se analiza el flujo de información mediante PCA y se reduce a los cuatro primero componentes logrando un 95\% de precisión.

Los CPGs codifican la información mediante circuitos neuronales~\cite{Grillner1985}, que se pueden analizar mediante diferentes tipos de osciladores aunque el más utilizado recientemente es el de Kuramoto. En el trabajo de~\cite{Dzeladini2014}, se utilizan osciladores morfados~\cite{Ajallooeian2013}. La implementación de ~\cite{Dzeladini2014} fue desarrollada en C++, escribiendo unas librerías para el sistema NMS. La optimización de los parámetros se llevó acabo con un algoritmo multi-criterio, ya que siempre se emplearon entre dos o más criterios. Los criterios principales fueron: minimización de la energía, penalidad por la hiperextensión de la rodilla, velocidad y longitud de paso. Se emplearon dos técnicas, Ordenamiento Lexicográfico para manejar el ordenamiento multiobejetivo de la función de \emph{fitness} sobre el frente de Pareto y la \emph{optimización basada en enjambres de partículas} PSO como estrategia de búsqueda principal.

Auke Ijspeert desarrolla un framework en~\cite{Ajallooeian2013}, dirigido al aprendizaje de la locomoción, denominado ``osciladores de fase no-lineales morfados''. Esta familia de osciladores incluye a los CPGs y a las primitivas de movimiento dinámicas DMPs~\cite{Ijspeert2002,Ijspeert2013} desarrolladas por inicialmente por Stefan Schaal. Esta es otra de las características de la encarnación de la inteligencia utilizando la computación morfológica, ya que este framework permite el diseño de sistemas dinámicos con distintos tipos de atractores. Sobre los Osciladores Morfados (MO) se construye una arquitectura Actor-Critica para el aprendizaje de la locomoción, compuesto de CPGs y DMPs para movimientos periódicos y discretos~\cite{Li2013a,Li2014}. La importancia de los MO radica en la reducción de la dimensionalidad, la cual es un problema de escalabilidad presente en el \emph{aprendizaje por refuerzo} (RL), permitiendo utilizar métodos de búsqueda de política directa o basada en gradiente como el actor-crítico~\cite{DeBroissia2016}.

Otro trabajo sobre la formación de reflejos sin la necesidad de CPGs, es presentado por Fumiya Iida en~\cite{Marques2013,Marques2014}, implementando un esquema de desarrollo intrínsecamente incremental, en donde prueba cuatro hipótesis sobre el desarrollo motor, que incluyen~\cite{Marques2014}: (1) a partir de las \emph{actividades motoras espontáneas} (SMA), presentes en el desarrollo fetal de los mamíferos, se puede generar reflejos, (2) que al interactuar con su morfología y entorno se forman de manera auto-organizada, (3) sobre los cuales en una modulación supraespinal de los reflejos, emergen comportamientos coordinados, que por último (4) demuestra que los tres pasos anteriores siguen estando activos durante el desarrollo y la vida del individuo, al probar aplicando cambios en la morfología con el fin de representar una lesión, para posterior a esto mostrar la capacidad de adaptación y robustez del sistema. Una similitud con el trabajo de~\cite{Song2015} a parte del sistema músculo-esquelético y el modelo de Hill, es la jerarquía del controlador, compuesta por dos etapas: pasiva y activa, en la etapa pasiva los reflejos proponen un comportamiento coordinado, pero dicho comportamiento puede ser modulado por la etapa activa mediante señales supraespinales. A diferencia del trabajo de Geyer, los reflejos son auto-organizados siguiendo un trabajo previo descrito en~\cite{Marques2013}, en donde la correlación de las señales sensoriales y motoras determinan los \emph{pathways} o patrones de conectividad de los diferentes circuitos reflexivos, la etapa pasiva es descrita en este artículo. En especial se habla de tres reflejos: reflejo de estiramiento, reflejo de inhibición recíproca y reflejo de estiramiento inverso\footnote{stretch o myotatic reflex, reciprocal inhibition reflex, reverse stretch o myotatic reflex}. El proceso de auto-organización basado en correlación, es un proceso no supervisado utilizando una regla anti-hebbiana conocida como anti-Oja.

Aplicaciones:

(Geyer)
Sobre prótesis transfemorales se estudia la recuperación del balance de la marcha utilizando un control NM~\cite{Thatte2015}, logrando una mayor robustez en la marcha comparado con los controladores de impedancia, los cuales se destacan en prótesis actuadas. Esta comparación se realiza mediante simulación y hardware, obteniendo como resultado una mejor comunicación entre el usuario y la prótesis, pero al final se plantea que es necesario estudiar mejor dicha política de control analizando la marcha anormal de un amputado.

\cite{Dzeladini2016}(Ijspeert)
Dzeladini, F. et al., 2016. Effects of a neuromuscular controller on a powered ankle exoskeleton during human walking. In Proceedings of the IEEE RAS and EMBS International Conference on Biomedical Robotics and Biomechatronics. pp. 617

\section{Computación morfológica y lazo sensoriomotor}
\label{sec:morphComp}

Las ideas de \emph{encarnación de la inteligencia}, planteadas en~\cite{Pfeifer2007}, donde se propone unos principios de diseño de inteligencia artificial encarnada, mencionan a la \emph{computación morfológica} (MC), como uno de los principales dominios de la Inteligencia sobre la Naturaleza, sobre los cuales la vida a través de la evolución y el desarrollo ha perfeccionado su cuerpo, permitiéndole adaptarse y permitir la emergencia de nuevos comportamientos, sobre los cuales el individuo puede interactuar mejor con el entorno en que subsiste. La alta riqueza no-lineal dinámica que está presente en el material y la morfología de los seres vivos, es demostrada en trabajos \emph{in-silico} de Fumiya Iida, donde se explora la dinámica natural y el flujo de información a través del cuerpo encontrado en el sistema músculo-esquelético~\cite{Iida2008,Iida2006}. 

Helmut Hauser siguiendo a Pfeifer, se plantea una capacidad de cómputo que es demostrada, realizada por el material y la morfología en~\cite{Hauser2012a,Hauser2012}, aquí el material y sus propiedades pasa a ser parte de la morfología, que están representados por un arreglo aleatorio de resortes no-lineales sobre los cuales se demuestra que por medio de una \emph{salida estática}\footnote{static readout: es el nombre utilizado en los artículos}, la morfología de un individuo es capaz de efectuar cómputos que involucran varios procedimientos como: cinemáticas inversas (IK) y directas (DK)~\cite{Li2013a}, dinámicas inversas (ID) y directas (DD)~\cite{Hauser2012}, cálculos multitarea y en paralelo~\cite{Hauser2012}, memorización de ciclos límite~\cite{Hauser2012a,Hauser2014}, memoria a corto plazo~\cite{Nakajima2014}, simplificación de las señales de control~\cite{Ghazi-Zahedi2015a} y otras más. El computador morfológico es definido mediante la existencia de unas propiedades como: (1) \emph{Separabilidad de entrada}, la cual es lograda mediante un mapeo no-lineal entre un espacio de baja dimensión a un espacio de alta dimensión, como en el método de las \emph{máquinas de soporte vectorial}. (2) \emph{Memoria desvaneciente} que consiste en sostener o mantener una secuencia de entrada reciente en el sistema, la cual permite integrar la información de los estímulos a través del tiempo sobre la parte importante de la señal, esta propiedad es explorada en~\cite{Nakajima2014}.

El trabajo de Hauser es soportado matemáticamente y fundamentado en (1) los sistemas dinámicos, (2) el control no-lineal realimentado y (3) la capacidad de cómputo. La relación de estos tres fundamentos previamente explicados por~\cite{Maass2007} de las redes neuronales en configuración de \emph{reservorio}, forman parte de un caso de la técnica \emph{computación reservorio} del \emph{aprendizaje de máquina} en donde el aprendizaje se realiza mediante una simple regresión lineal~\cite{Hauser2014}. El arreglo de resortes propuesto por Hauser es denominado un \emph{reservorio físico} y de forma formal se da una definición matemática de computación y control morfológico reuniendo los trabajos anteriores en~\cite{Fuchslin2013,Hauser2013}, en donde se destacan diferentes aplicaciones de biomecánica y medicina utilizando la mecánica clásica y la mecánica estadística.

La \emph{morfosis} o \emph{morfología adaptativa}~\cite{Hauser2017}, es el concepto más reciente que se estudia en MC, mediante esta propiedad del sistema, el cambio de los parámetros que definen la morfología permiten el cambio de atractores dinámicos, los cuales representan o codifican un determinado comportamiento, sin la necesidad de cambiar la señal de control. En lugar de cambiar la señal de control al presentarse un cambio en el entorno, la morfología se adapta logrando configuraciones de comportamientos eficientes cuando el sistema permite la morfosis. Por lo tanto la morfosis permite operar en escenarios ruidosos y proponer investigaciones futuras en estructuras jerárquicas de control de alto nivel, el crecimiento artificial de la estructura y los sistemas auto-curativos. Las ideas de morfosis fueron implementadas por~\cite{Vu2013}, en una plataforma robótica mono-pédica basada en un sistema biela-manivela de \emph{actuación de rigidez variable} (VSA), inmersa en un ambiente variable de escalones, pendientes y diferentes rugosidades del piso, sobre los cuales el comportamiento de la señal de control para lograr el movimiento se mantuvo fijo y se exploró los parámetros del mecanismo biela-manivela, sobre el cual se obtuvo comportamientos adecuados para cada configuración del piso y mostrando un desempeño eficiente. Otra implementación de morfosis es el ya mencionado trabajo de~\cite{Song2015}.

Con los trabajos anteriores se demostró la presencia de la computación morfológica en la locomoción y el comportamiento de las especies. Las tendencias de investigación de esta área se dirigieron hacia la formación del comportamiento de forma autónoma por parte del \emph{agente}~\cite{Martius2014}. El carácter estocástico, la auto-organización y el flujo de información entre el agente y el entorno son ahora los principales temas estudiados. El problema del MC tomó herramientas del control óptimo estocástico, el aprendizaje por refuerzo y la teoría de la información. A continuación se resume algunos trabajos de grupos de investigadores con respecto al área de MC y sus nuevos problemas.

Continuando con las ideas de MC~\cite{Pfeifer2007}, en \cite{Ruckert2012} el propósito principal es, cómo cuantificar la cantidad de control que el sistema robótico le delega a la morfología. Para esto una cadena serial de cuatro eslabones, es utilizada para modelar el balance de un humanoide aplicando torque en diferentes articulaciones (tobillos, rodillas, cadera y brazos), además el balance se debe mantener en presencia de empujones externos. Los parámetros que forman parte de la morfología, los cuales son explorados son: la fricción de articulación, lo longitud de los eslabones e inspirado por el trabajo de~\cite{Hauser2012}, se utilizaron resortes lineales cuyo acople con los eslabones permitían un comportamiento no-lineal en el torque de la articulación. Se utilizó CMA-ES para la búsqueda de la morfología. La idea principal es: (1) utilizar el \emph{Control Óptimo Estocástico} (SOC) para proponer leyes de control sobre cualquier morfología y (2) a partir de esto cambiar la morfología con el fin de reducir la complejidad de la señal de control y lograr un alto desempeño, obteniendo así un control y morfología óptimos. Variación de las ganancias de control es relacionada con la cantidad de cómputo del controlador. Generalmente el problema SOC~\cite{Toussaint2009}, es resuelto para casos lineales mediante LQR, cuando el problema tiene incertidumbres se utiliza LQG, pero cuando el problema es además no-linealse puede utilizar iLQG, que puede ser aplicado para problemas del caso LQG. En el trabajo de~\cite{Ruckert2012}, el SOC es planteado mediante un modelo gráfico, consistente en una red bayesiana causal, el método es denominado \emph{Control de inferencia aproximada} (AICO)~\cite{Toussaint2009} y puede ser extendido a casos no-LQG, en donde el modelo dinámico del sistema y la función de costo son representados por el modelo gráfico, la inferencia del la red resulta en la política de control.

El siguiente trabajo de Rückert~\cite{Ruckert2012a}, utiliza el modelo gráfico anteriormente mencionado, para plantear un modelo de primitivas de movimiento (MP) capaz de proveer al sistema con un mecanismo de planeación probabilística, denominado \emph{Primitivas de movimiento de planeación} (PMP). Los aportes del PMP son: (1) al igual que los DMP aporta modularidad al sistema, (2) mejorando a los DMP, aporta optimalidad estocástica ante perturbaciones y (3) tiene eficiencia para el aprendizaje. Como novedad al trabajo anterior el modelo gráfico, el modelo dinámico y la función de costo son aprendidos mediante RL. Este trabajo involucra los tres tipos aprendizaje RL: basado en modelo, libre de modelo y aprendizaje de modelo. A diferencia de los DMP que parametrizan las trayectorias codificándolas como atractores dinámicos, los PMP parametrizan la función de costo intrínseca del SOC, que difiere de la función de recompensa, para buscar políticas mediante un mecanismo de inferencia, luego probarlas en el modelo real, utilizar la información del modelo real para aproximar el modelo dinámico, y con el modelo dinámico interno realizar planeación sin requerir de un re-aprendizaje. En este trabajo se muestra la importancia de definir una tarea mediante características relevantes para facilitar el aprendizaje y generalización de habilidades de movimiento complejas.

En \cite{Ruckert2013}, se utiliza una modificación del \emph{framework} de los DMPs~\cite{Ijspeert2013}, que inspirado por hipótesis sobre el sistema de control neuromuscular implementa sinergias en las señales de control con el fin de reducir la dimensión del problema~\cite{DAvella2003}, acelerar el aprendizaje de tareas nuevas y reutilizar el conocimiento adquirido de otras tareas al estructurar el controlador en dos tipos de parámetros: (1) un conjunto de parámetros independientes de tareas, que representan señales de grupos musculares que forman funciones bases, (2) las funciones base o sinergias, se pueden correr en el tiempo y amplificar en magnitud mediante otro conjunto más pequeño de parámetros que es utilizado para desarrollar una tarea específica. Este método denominado DMPSynergies se destaca en que, es más eficiente para el aprendizaje y permite aprendizaje multitarea de forma simultánea al compararse con los DMPs, las sinergias son formadas a través del RL y permite estructurar las sinergias en su complejidad al estudiar de forma incremental la composición de las sinergias, definiendo así una medida de la complejidad del controlador. Las no-linealidades sobre la política complican el aprendizaje mediante imitación comparado con los DMPs. En este trabajo de Rückert se utiliza el modelo NMS basado de un brazo utilizando unidades de Hill realizando tareas de alcance en donde se muestra una representación esparcida de las señales de control que coinciden con los datos de actividad electromiográfica y la habilidad de las sinergias para la generalización.

Nihat Ay plantea el lazo sensoriomotor mediante un modelo gráfico (o red bayesiana) causal, en donde los fenómenos estocásticos de la interacción de un agente y su entorno, pueden ser modelados y las políticas o estrategias de control, pueden ser buscadas mediante el aprendizaje por refuerzo y el control óptimo estocástico pero donde la información que fluye a través del lazo debe de ser organizada. Modelos de CRBM.

Elmar Rückert, Calanca y Peters, modelo PMP, redes bayesianas.

Nihat Ay~\cite{Ay2012}, comenta cómo la teoría de la información ha sido utilizada recientemente para el estudio de la dinámica del lazo sensoriomotor de los robots y los seres vivos, tomándolos como sistemas de procesamiento de información, sobre los cuales puede formarse mecanismos de curiosidad e innovación. El problema principal es como utilizar la información sensorial que produce las acciones del robot al interactuar con el entorno de forma que se pueda afectar las acciones y percepciones futuras. La información predictiva (PI) es propuesta para cuantificar la totalidad de la información de las experiencias pasadas que puede ser usada para predecir los eventos futuros. Con ejemplos lineales se concluye que el principio de la \emph{maximización de la información predictiva} (PIMAX) es una herramienta versátil para la auto-organización de comportamientos en sistemas robóticos complejos. La adaptación de los controladores, es decir la maximización de la información predictiva, se puede realizar mediante procesos de evolución o desarrollo, conduciendo a dos formas de representación: evolutiva y de aprendizaje en línea.

Continuando el trabajo anterior~\cite{Ay2012} y el trabajo de homeokinesis de~\cite{Der2012}, ~\cite{Martius2013} aplica ahora los resultados para modelos no-lineales y no-estacionarios, introduce el concepto de \emph{información predictiva de tiempo local} (TiPI), además las reglas de actualización de los parámetros de controlador son planteadas de forma que el principio de maximización de la información queda encarnado en las dinámicas sinápticas del sistema. El TiPI es demostrados sobre simulaciones físicas de sistemas de alta dimensionalidad, los cuales logran encarnación, motivación intrínseca y espontaneidad\footnote{Relacionado con auto-determinación}. Las no-linealidad producen en los comportamientos, cooperación simultánea y auto-switching de dinámicas en sistemas con histéresis simples. Posterior a este trabajo, se prueba PIMAX sobre una plataforma robótica real utilizando TiPI~\cite{Martius2014}. Estos trabajos muestran como PI es un buen candidato para el aprendizaje ilimitado y autónomo de comportamientos explorando la dependencia del controlador a la morfología y el entorno~\cite{Zahedi2013}. 

Exploración de redes bayesianas.

\section{Cognición, aprendizaje y toma de decisiones}
\label{sec:cognition}

Lazo sensorio motor y teoría de la información, mediadas de información, cantidad mínima de información requerida para que un agente pueda maximizar una función de utilidad, reglas para el aprendizaje autónomo y no-supervisado.

En \cite{Kober2013}, el RL es reseñado para los sistemas robóticos mostrando sus principales retos y diferencias con: (1) el RL procedente del aprendizaje de máquina, en donde el problema es resuelto principalmente a partir de su formulación dual, utilizando funciones de valor que aproximan el retorno o recompensa total mediante las ecuaciones de Bellman y (2) el control óptimo, que resuelve el problema original, mediante búsqueda de políticas frecuentemente basadas en el modelo y generación de trayectorias. Los sistemas robóticos no se pueden considerar como completamente observables o libres de ruido, es decir no es un problema MDP determinístico, en realidad son casi un POMDP estocástico, es decir MDPs parcialmente observables, no obstante son un poco más complejos ya que la propiedad markoviana no se asegura por los retardos en las señales de actuación y sensado. Los sistemas de aprendizaje robóticos utilizan filtros para estimar el estado real. Es esencial que, la información del estado tenga además del valor sensado, una noción de incertidumbre sobre sus estimados. Esto hace que la experiencia sobre un sistema real sea difícil y costosa de obtener y de reproducir. Además, a diferencia del RL tradicional en donde la señal de recompensa frecuentemente es \emph{sparse}, el diseño de la recompensa en el RL robótico, denominado \emph{reward shaping}, que se utiliza para acelerar exitosamente el aprendizaje, requiere de una gran cantidad de conocimiento en el dominio para su implementación lo cual hace difícil su práctica. El problema entre \emph{exploración y explotación}, el cual permite explorar el espacio estado-acción puede ser intratable pero dedicarse solo a explotación puede conducir a óptimos locales, los cuales pueden resultar en desempeños de controladores pobres. El problema de \emph{la maldición de la dimensionalidad}, que debido a los sistemas robóticos humanoides, que fácilmente alcanzan 100 DoFs, y donde el espacio estado-acción es continuo hacen nuevamente al RL robótico intratable. Los métodos \emph{on-policy} recolectan la información usando una política actual, y como resultado la exploración se construye sobre la política y esta determina la velocidad de mejoramiento de la política, obteniendo mejoramientos a corto y largo plazo. Modelar la exploración mediante modelos probabilísticos permiten la tratabilidad de la alta dimensionalidad además hace que el paso de mejoramiento de la política sea sencillo. Los métodos basados en \emph{búsqueda de políticas} (PS) permiten de forma natural la integración del conocimiento, que a su vez permite la pre-estructuración apropiada para un dominio especifico de la forma aproximada de la política sin cambiar el problema original. Además permite incorporar de forma natural restricciones adicionales al problema. El uso de funciones de aproximación procedentes de los método se aprendizaje supervisado, están diseñados bajo la suposición de que las muestras de datos son \emph{independientes e idénticamente distribuidos} (\emph{i.i.d}), enfrentan en el RL que, los datos proceden de los sistemas robóticos no son ni independientes y ni tampoco idénticamente distribuidos. 

Los retos del RL robótico son: la maldición de la dimensionalidad, muestreo del mundo real, sub-modelado y modelado de incertidumbres, especificación del objetivo. En RRL, se considera más importante limitar el tiempo de interacción con el mundo real en lugar de limitar el consumo de memoria y la complejidad computacional. Por lo tanto los algoritmos de eficiencia muestral que son capaces de aprender en un pequeño número de intentos o corridas es esencial. Todos los sistemas físicos exhiben retardos de actuación y sensado. Debido a estos retardos, las acciones puede que no tengan efectos instantáneos que son observables solo varios intervalos de tiempo posteriores, estos retardos violan supuestos de MDPs. Para tareas donde los sistemas son auto-estabilizantes, es decir que no requieren de un control activo para llegar o regresar a una posición de seguridad, generalmente la transferencia de políticas simuladas trabaja bien en el mundo real. Cuando el sistema físico real realiza una tarea  auto-estabilizante los algoritmos se puede utilizar directamente. En tareas inestables una pequeña variación puede tener consecuencias drásticas y la transferencia de políticas resulta en malos desempeños. Utilizar modelos aproximados sirve para: verificar y probar algoritmos en simulación, establecer proximidad a soluciones teóricamente óptimas, calcular gradientes aproximados para el mejoramiento de políticas locales, identificar estrategias para recolectar más datos y realizar \emph{mental rehearsal}. En lugar de proponer funciones de recompensa simples binarias, frecuentemente se incluye recompensas intermedias, para guiar un proceso de aprendizaje a una solución razonable, mediante \emph{reward shaping}. Existe un intercambio entre la complejidad de la función de recompensa y la complejidad del problema de aprendizaje. El comportamiento humano que parece respetar las prioridades de tiempo y riesgo, puede lograrse mediante una señal de recompensa que describa el mapeo de las características que describen cada estado. Cuando las funciones recompensa son lineales en el conjunto de características, esas funciones imponen una limitación restrictiva sobre la solución del problema IRL. La noción de construir políticas complejas a partir de unas más simples, las cuales son fácilmente resueltas por el control óptimo al explorar funciones de recompensa parametrizadas. Políticas complejas derivadas de la adaptación de una función de recompensas para problemas simples de control óptimo han sido utilizada en conjunto con PS.

Las técnicas tradicionales de RL probablemente estén condenadas a fallar por las consecuencias computacionales y del manejo de la información presentes en el RL robótico. Para la tratabilidad de los retos del problema RL robótico se utiliza: a través de la representación, a través de la aproximación de modelos, el manejo de conocimiento a-priori e información. El conocimiento a-priori puede ser incluido mediante: políticas iniciales, demostraciones, modelos iniciales, estructuras de tarea predefinidas, o restricciones sobre políticas (e.g. como limites de torque), con el fin de reducir los espacios de búsqueda y acelerar el proceso de aprendizaje. Pre-estructurar una tarea compleja en pequeños problemas más sencillos, puede reducir la complejidad de la tarea de aprendizaje. Las restricciones pueden limitar el espacio de búsqueda pero también involucran problemas adicionales, que generalmente se relajan en la búsqueda pero aquí esta no es una opción. La combinación de aprendizaje por imitación y RL, denominado \emph{aprendizaje de principiante}, se caracteriza por la necesidad de un profesor y la practica realizada por el aprendiz. Es posible a partir de las demostraciones encontrar óptimos locales que nos permitan tomar las demostraciones como políticas iniciales de partida. Muchas veces las demostraciones humanas a humanoides deben ser adaptadas teniendo en cuenta las diferencias cinemáticas entre el profesor y el robot. Otras veces el profesor controla directamente el robot  poder obtener una demostración.

RL jerárquico utilizando meta-acciones, que corresponde al manejo de tareas de bajo nivel mediante una capa de alto nivel que coordina mediante estrategias. Por ejemplo, el ponerse de pie se obtiene mediante dos capas, \emph{Q-learning} en la capa de estrategia y \emph{actor-critic} en el bajo nivel. Este RL jerárquico ha sido empleado con éxito en el controladores de marcha de humanoides encontrando controladores óptimos a partir de coordinar subsistemas más simples. Inspirados por los sistemas biológicos que aprenden de forma progresiva el desarrollo de sus tareas, se utiliza el \emph{reward shaping} de tal forma que el término de balance domine sobre los otros términos como los de orientación a un objetivo y por lo tanto se logra que el balance se aprenda en primera instancia. El conocimiento a-priori de la tarea para mecanismos de curiosidad que exploren regiones nuevas y prometedoras. Muchos problemas de RRL pueden ser tratables al aprender modelos directos, son modelos dinámicos de transición que se soportan sobre los datos y permiten entrenar los sistemas con una mejor interacción con el entorno real. Combinar el aprendizaje de simulación y el sistema real para reducir la experimentación real se conoce como \emph{mental rehearsal}. \emph{Simulation Biases}, surge de la imposibilidad de obtener un modelo directo, para simular con precisión el sistema real robótico si error, resultando en políticas que funcionan bien en el modelo directo pero pobremente en el sistema real. Promediar sobre el modelo de incertidumbre en un modelo probabilístico puede ser usado para reducir el \emph{bias}. Frecuentemente introducir modelos estocásticos o distribuciones sobre los modelos, permite enfrentar el bias de simulación. Añadir ruido artificial a las señales permite suavizar los errores del modelo y evadir el sobre-ajuste de la política. Los métodos de aprendizaje de modelo que manejan una incertidumbre probabilística alrededor de la verdadera dinámica del sistema permiten al algoritmo de RL generar distribuciones sobre el desempeño de la política. Al tratar la incertidumbre como si fuera ruido y empleando una aproximación analítico de una simulación directa, la tarea de elevar un cart-pole puede ser resuelta en menos de 20 segundos de interacción con el modelo real. Muestreando y re-utilizando números aleatorios, un modelo directo puede ser usado en simulación para crear \emph{roll-outs} para entrenar por muestreo. Cuando se compara los resultados de diferentes corridas de simulación, es realmente difícil, decir el número más pequeño de muestras para que una política trabaje mejor o solo sea el efecto de la estocasticidad del modelo simulado.

Como se resume en~\cite{Zahedi2010}, la cognición de un sistema, hace referencia al proceso que transforma los datos sensoriales en comandos motores usando alguna forma de representación interna no-simbólica, y por lo tanto la cognición es un proceso el cual habita en el lazo sensoriomotor produciendo adaptación y desarrollo al sistema que lo contiene. En este trabajo se buscó una regla de aprendizaje no-supervisado y auto-organizado general, que utilizara el lazo sensoriomotor, de forma que el agente no requiriera del conocimiento de un modelo estructural y suposiciones previas del entorno y la morfología, dicha regla esta basada sobre el principio de la maximización de la información.

Se extienden los resultados de~\cite{Martius2013} para RL~\cite{Zahedi2013}, en donde los mecanismos de aprendizaje de comportamientos de PI, se utilizaron como motivación intrínseca impulsada por información lazo sensoriomotor que  \emph{no es dependiente de una tarea especifica}\footnote{task-independent}, para proponer un aprendizaje que si es dirigido a la realización de una tarea específica, es decir \emph{dependiente de tareas}\footnote{task-dependent}. Se combinan la señal de PI como motivación intrínseca (IRF), con una señal de motivación extrínseca (ERF), utilizando un algoritmo de búsqueda de políticas episódico.

La Cinemática Inversa (IK) No Requiere de ser calculada bajo la lógica morfológica~\cite{Li2013a},

Representación y selección de características para la locomoción y el comportamiento coordinado. Definición de Aprendizaje de Maquina.

Introducción del Deep Learning en RL. Trabajos relevantes.

Distributed representation and factors of variation.

La locomoción como un modelo de secuencias de posturas y su relación con los LSTM.

De la computación gráfica, en la animación de personajes, recientemente el trabajo de~\cite{Agrawal2016,Liu2016,Holden2016} y el trabajo de~\cite{Holden2016}. DNN para el control de movimientos, aproximación directa de la política, Deep Q-Learning que encuentra dificultad en espacios continuos,
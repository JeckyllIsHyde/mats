\chapter{Antecedentes}
\label{chp:ante}

\section{Modelo Neuro-músculo-esquelético}
\label{sec:modNMS}

Tal vez el modelo del sistema neuro-músculo-esquelético (NMS), que más impacto ha tenido recientemente, propuesto por~\cite{Geyer2010}, es un modelo desarrollado en MATLAB y SimMechanics, proponiendo una locomoción coordinada de la caminata utilizando un conjunto de reflejos, que actúan sobre un conjunto de músculos basados en el modelo de Hill propuesto en~\cite{Geyer2003}, donde se estudia la relación fuerza-longitud-velocidad del músculo y los diferentes circuitos (o \emph{pathways}) que se pueden proponer bajo ese modelo. Los músculos se organizan formando sinergias musculares~\cite{DAvella2003}, de forma que permiten al sistema esquelético: evitar hiperextenciones de las articulaciones, agregar flexibilidad (\emph{Compliance}), evadir el choque con el piso en la etapa de balanceo, reciclar la energía en el talón y otras. Se utiliza algoritmos genéticos (GAs) para la parametrización del sistema NMS.

Posterior a este modelo, Geyer continuó el modelo esta vez para un sistema tridimensional~\cite{Song2015} añadiendo nuevos músculos y reflejos, en donde emergen distintos comportamientos de marcha en modo de: caminar, correr, aceleración, desaceleración, negociación de pendientes y escalones, giro, y evasión deliberada de obstáculos. El controlador es un sistema jerárquico compuesto por dos niveles, en el primero se almacenan y coordinan los reflejos y en el segundo se controla el posicionamiento del pie. El proceso de optimización de los parámetros es cambiado por otra técnica evolutiva recientemente utilizada como método de búsqueda de políticas directa, denominada \emph{Covariance Matrix Adpatation-Evolution Strategy} CMA-ES~\cite{Hansen2006}.

Paralelo al trabajo de~\cite{Geyer2010,Song2015}, André Seyfarth continuo desde~\cite{Geyer2003}, con un trabajo sobre el modelo muscular y su importancia en la comunicación de las señales realimentadas y prealimentadas en la adaptación a un entorno cambiante o con cierto nivel de incertidumbre~\cite{Haeufle2012}. Dentro del canal de comunicación, las señales aferentes provenientes de las fibras sensoriales son estudiadas en detenimiento para el caso de un movimiento periódico de salto. Los husos neuromusculares como receptores sensoriales, llevan la información propioceptiva al controlador, estos son: (1) Longitud y velocidad de alargamiento y contracción del músculo y (2) los \emph{órganos tendinosos de Golgi} llevan la información de la fuerza. Este canal es estudiado concluyendo la importancia de cada una de estas señales en la estabilidad y control del salto, además de especificar si la realimentación es positiva o negativa.

El siguiente paso de Haeufle, al comprobar que las no-linealidades eran útiles en el modelo muscular de~\cite{Haeufle2012}, fue cuantificar la información que fluía por el canal al mirar que tanto computo debía realizar el controlador para lograr una acción específica~\cite{Haeufle2014}, para ello utilizó \emph{la entropía de la información de Shannon}~\cite{MacKay2005}. Ya metido en el campo de la computación morfológica comparó tres modelos de robots saltadores simulados al mirar la complejidad del control en cada uno de los tres siguientes casos: (1) modelo no-lineal del músculo de Hill, (2) modelo lineal del músculo de Hill y (3) motor DC como actuador~\cite{Ghazi-Zahedi2015a}. Otros modelos de músculos basados en Hill son propuestos en~\cite{Schmitt2015}, complementan mejor la comparación de un control real.

Extendiendo el trabajo de~\cite{Geyer2010} y paralelo a~\cite{Song2015}, Auke Ijspeert, conocido por su trabajo en CPGs~\cite{Ijspeert2008}, muestra la necesidad de incluir los CPGs en la estructura puramente reflexiva de Geyer en~\cite{Dzeladini2014}. Aunque el modelo de Geyer es estable en presencia de perturbaciones, la modulación de la velocidad y el tamaño del paso no son problemas sencillos y requieren de procesos de optimización \emph{offline} demorados. Al incluir los CPGs en el modelo NMS, la modulación de la velocidad y el paso de la marcha se tornan sencillas y además se muestra que los CPGs pueden ser utilizados como modelos predictivos del lazo sensoriomotor, al ser capaces de reproducir las señales aferentes generadas por una marcha estable. Esta capacidad predictiva de los CPGs ofrece una forma de comparar la importancia relativa de distintos circuitos o pathways de realimentación. Como trabajos futuros, se propone un estudio de co-evolución de las componentes de realimentación y prealimentación. Adicional se concluye que las señales de los pathways pueden ser estudiadas mejor a través de las señales de las motoneuronas, formando señales de baja dimensión compuesta por cuatro primitivas motrices controlando con una mayor facilidad la velocidad, longitud de paso, transición de marchas y la adaptación a pendientes cambiantes. La reducción de dimensionalidad expresada en primitivas de movimiento, de nuevo es estudiada por Ijspeert en~\cite{Sprowitz2014a}, donde se analiza el flujo de información mediante PCA\footnote{Análisis de componentes principales (PCA)} y se reduce a los cuatro primero componentes logrando un 95\% de precisión.

Los CPGs codifican la información mediante circuitos neuronales~\cite{Grillner1985}, que se pueden analizar mediante diferentes tipos de osciladores aunque el más utilizado recientemente es el de Kuramoto. En el trabajo de~\cite{Dzeladini2014}, se utilizan osciladores morfados~\cite{Ajallooeian2013}. La implementación de ~\cite{Dzeladini2014} fue desarrollada en C++, escribiendo unas librerías para el sistema NMS. La optimización de los parámetros se llevó acabo con un algoritmo multi-criterio, ya que siempre se emplearon entre dos o más criterios. Los criterios principales fueron: minimización de la energía, penalidad por la hiperextensión de la rodilla, velocidad y longitud de paso. Se emplearon dos técnicas, Ordenamiento Lexicográfico para manejar el ordenamiento multiobejetivo de la función de \emph{fitness} sobre el frente de Pareto y la \emph{optimización basada en enjambres de partículas} PSO como estrategia de búsqueda principal.

Auke Ijspeert desarrolla un framework en~\cite{Ajallooeian2013}, dirigido al aprendizaje de la locomoción, denominado ``osciladores de fase no-lineales morfados''. Esta familia de osciladores incluye a los CPGs y a las primitivas de movimiento dinámicas DMPs~\cite{Ijspeert2002,Ijspeert2013} desarrolladas inicialmente por Stefan Schaal. Esta es otra de las características de la encarnación de la inteligencia utilizando la computación morfológica, ya que este framework permite el diseño de sistemas dinámicos con distintos tipos de atractores. Sobre los Osciladores Morfados (MO) se construye una arquitectura Actor-Critica para el aprendizaje de la locomoción, compuesto de CPGs y DMPs para movimientos periódicos y discretos~\cite{Li2013a,Li2014}. La importancia de los MO radica en la reducción de la dimensionalidad, la cual es un problema de escalabilidad presente en el \emph{aprendizaje por refuerzo} (RL), permitiendo utilizar métodos de búsqueda de política directa o basada en gradiente como el actor-crítico~\cite{DeBroissia2016}.

Otro trabajo sobre la formación de reflejos sin la necesidad de CPGs, es presentado por Fumiya Iida en~\cite{Marques2013,Marques2014}, implementando un esquema de desarrollo intrínsecamente incremental, en donde prueba cuatro hipótesis sobre el desarrollo motor, que incluyen~\cite{Marques2014}: (1) a partir de las \emph{actividades motoras espontáneas} (SMA), presentes en el desarrollo fetal de los mamíferos, se puede generar reflejos, que (2) al interactuar con su morfología y entorno se forman de manera auto-organizada, (3) sobre los cuales en una modulación supraespinal de los reflejos, emergen comportamientos coordinados, que por último (4) demuestra que los tres pasos anteriores siguen estando activos durante el desarrollo y la vida del individuo, al probar aplicando cambios en la morfología con el fin de representar una lesión, para posterior a esto mostrar la capacidad de adaptación y robustez del sistema. Una similitud con el trabajo de~\cite{Song2015} a parte del sistema músculo-esquelético y el modelo de Hill, es la jerarquía del controlador, compuesta por dos etapas: pasiva y activa, en la etapa pasiva los reflejos proponen un comportamiento coordinado, pero dicho comportamiento puede ser modulado por la etapa activa mediante señales supraespinales. A diferencia del trabajo de Geyer, los reflejos son auto-organizados siguiendo un trabajo previo descrito en~\cite{Marques2013}, en donde la correlación de las señales sensoriales y motoras determinan los \emph{pathways} o patrones de conectividad de los diferentes circuitos reflexivos, la etapa pasiva es descrita en ese artículo. En especial se habla de tres reflejos: reflejo de estiramiento, reflejo de inhibición recíproca y reflejo de estiramiento inverso\footnote{stretch o myotatic reflex, reciprocal inhibition reflex, reverse stretch o myotatic reflex}. El proceso de auto-organización basado en correlación, es un proceso no supervisado utilizando una regla anti-hebbiana conocida como anti-Oja.

\section{Computación morfológica y lazo sensoriomotor}
\label{sec:morphComp}

Las ideas de \emph{encarnación de la inteligencia}, planteadas en~\cite{Pfeifer2007}, donde se propone unos principios de diseño de inteligencia artificial encarnada, mencionan a la \emph{computación morfológica} (MC), como uno de los principales dominios de la Inteligencia sobre la Naturaleza, sobre los cuales la vida a través de la evolución y el desarrollo ha perfeccionado su cuerpo, permitiéndole adaptarse y permitir la emergencia de nuevos comportamientos, sobre los cuales el individuo puede interactuar mejor con el entorno en que subsiste. La alta riqueza no-lineal dinámica que está presente en el material y la morfología de los seres vivos, es demostrada en trabajos \emph{in-silico} de Fumiya Iida, donde se explora la dinámica natural y el flujo de información a través del cuerpo encontrado en el sistema músculo-esquelético~\cite{Iida2008,Iida2006}. 

Helmut Hauser siguiendo a Pfeifer, se plantea una capacidad de cómputo que es demostrada, realizada por el material y la morfología en~\cite{Hauser2012a,Hauser2012}, aquí el material y sus propiedades pasa a ser parte de la morfología, que están representados por un arreglo aleatorio de resortes no-lineales sobre los cuales se demuestra que por medio de una \emph{salida estática}\footnote{static readout: es el nombre utilizado en los artículos}, la morfología de un individuo es capaz de efectuar cómputos que involucran varios procedimientos como: cinemáticas inversas (IK) y directas (DK)~\cite{Li2013a}, dinámicas inversas (ID) y directas (DD)~\cite{Hauser2012}, cálculos multitarea y en paralelo~\cite{Hauser2012}, memorización de ciclos límite~\cite{Hauser2012a,Hauser2014}, memoria a corto plazo~\cite{Nakajima2014}, simplificación de las señales de control~\cite{Ghazi-Zahedi2015a} y otras más. El computador morfológico es definido mediante la existencia de unas propiedades como: (1) \emph{Separabilidad de entrada}, la cual es lograda mediante un mapeo no-lineal entre un espacio de baja dimensión a un espacio de alta dimensión, como en el método de las \emph{máquinas de soporte vectorial}. (2) \emph{Memoria desvaneciente} que consiste en sostener o mantener una secuencia de entrada reciente en el sistema, la cual permite integrar la información de los estímulos a través del tiempo sobre la parte importante de la señal, esta propiedad es explorada en~\cite{Nakajima2014}.

El trabajo de Hauser es soportado matemáticamente y fundamentado en (1) los sistemas dinámicos, (2) el control no-lineal realimentado y (3) la capacidad de cómputo. La relación de estos tres fundamentos previamente explicados por~\cite{Maass2007} de las redes neuronales en configuración de \emph{reservorio}, forman parte de un caso de la técnica \emph{computación reservorio} del \emph{aprendizaje de máquina} en donde el aprendizaje se realiza mediante una simple regresión lineal~\cite{Hauser2014}. El arreglo de resortes propuesto por Hauser es denominado un \emph{reservorio físico} y de forma formal se da una definición matemática de computación y control morfológico reuniendo los trabajos anteriores en~\cite{Fuchslin2013,Hauser2013}, en donde se destacan diferentes aplicaciones de biomecánica y medicina utilizando la mecánica clásica y la mecánica estadística.

La \emph{morfosis} o \emph{morfología adaptativa}~\cite{Hauser2017}, es el concepto más reciente que se estudia en MC, mediante esta propiedad del sistema, el cambio de los parámetros que definen la morfología permiten el cambio de atractores dinámicos, los cuales representan o codifican un determinado comportamiento, sin la necesidad de cambiar la señal de control. En lugar de cambiar la señal de control al presentarse un cambio en el entorno, la morfología se adapta logrando configuraciones de comportamientos eficientes cuando el sistema permite la morfosis. Por lo tanto la morfosis permite operar en escenarios ruidosos y proponer investigaciones futuras en estructuras jerárquicas de control de alto nivel, el crecimiento artificial de la estructura y los sistemas auto-curativos. Las ideas de morfosis fueron implementadas por~\cite{Vu2013}, en una plataforma robótica mono-pédica basada en un sistema biela-manivela de \emph{actuación de rigidez variable} (VSA), inmersa en un ambiente variable de escalones, pendientes y diferentes rugosidades del piso, sobre los cuales el comportamiento de la señal de control para lograr el movimiento se mantuvo fijo y se exploró los parámetros del mecanismo biela-manivela, sobre el cual se obtuvo comportamientos adecuados para cada configuración del piso y mostrando un desempeño eficiente. Otra implementación de morfosis es el ya mencionado trabajo de~\cite{Song2015}.

Con los trabajos anteriores se demostró la presencia de la computación morfológica en la locomoción y el comportamiento de las especies. Las tendencias de investigación de esta área se dirigieron hacia la formación del comportamiento de forma autónoma por parte del \emph{agente}~\cite{Martius2014}. El carácter estocástico, la auto-organización y el flujo de información entre el agente y el entorno son ahora los principales temas estudiados. El problema del MC tomó herramientas del control óptimo estocástico, el aprendizaje por refuerzo y la teoría de la información. A continuación se resume algunos trabajos de grupos de investigadores con respecto al área de MC y sus nuevos problemas.

El profesor Emo Todorov, destacado por sus trabajos de optimalidad del lazo sensoriomotor~\cite{Todorov2002,Todorov2004,WeiweiLi2004}, el motor físico MuJoCo~\cite{Todorov2012} y contribuciones en aplicaciones al \emph{Deep Reinforcement Learning} para el control continuo en el trabajo de Sergei Levine; utiliza las herramientas de Control Óptimo para demostrar propiedades del lazo sensoriomotor presentes en los seres vivos. En su trabajo~\cite{Todorov2004}, se describe el \emph{sistema sensoriomotor} como el resultado de procesos de optimización como: la evolución, el desarrollo, el aprendizaje y la adaptación; los cuales trabajan en diferentes escalas de tiempo para mejorar el desempeño del comportamiento. Estos comportamientos se describen mediante unos objetivos que conforman las tareas, las cuales se describen o ``codifican'' como funciones de costo. La función de costo es escalar y depende de las señales de control y las variables que describen el sistema NMS acoplado al entorno. Generalmente se optimiza varios criterios en simultáneo como: el jerk, el torque, el cambio del torque, la energía, el tiempo, la varianza y otros más; para obtener mejores desempeños en una tarea específica, sin embargo cada criterio se puede estudiar por separado para investigar su relevancia individual sobre una tarea específica. 

El diseño de controladores óptimos realimentados o de control óptimo estocástico, según~\cite{Todorov2004}, que toma en cuenta el ruido presente en la señal de actuación, la incertidumbre sensorial y los retardos; permite además unificar varios conceptos como: regularidades cinemáticas, sinergias motrices y parámetros controlados, control del efector final, redundancia motora y variabilidad estructurada, control de impedancia, compensaciones entre rapidez y precisión; que a su vez permiten plantear el control sensoriomotor desde un punto de vista jerárquico, explorando por ejemplo modelos neurales de la espina y de funciones de la corteza motriz relevantes en las propiedades cognitivas. Comúnmente la observabilidad de sistemas robóticos, solo es posible con sensores ruidosos y con retardos. El controlador debe operar sobre una estimación interna del estado y las leyes de control solo son óptimas, cuando el estimador de estado también es óptimo, y por lo general se soporta sobre la \emph{Inferencia Bayesiana}. Éstos estimadores, hacen su inferencia basados en la información de las señales del lazo sensoriomotor, donde se toman: la señal sensorial como es, la señal de control reciente, el conocimiento de la dinámica del cuerpo, así como las salidas anteriores más inmediatas. El filtro de Kalman, el cual es un estimador óptimo cuando la dinámica y las mediciones sensoriales son lineales y el ruido es Gaussiano, es un ejemplo de ello. Los estimadores óptimos anticipan la señal sensorial antes de que ocurra, lo cual es visto como un modelo interno del sistema, es decir una dinámica directa. El controlador obedece a la señal del estimador. Las capacidades predictivas del estimador permiten al controlador contrarrestar las perturbaciones antes de que se provoquen los errores, esto se logra enviando señales de fuerza muscular o por ajustar la co-activación muscular al variar parte de su morfología. 

La redundancia~\cite{Todorov2004} hace referencia a la capacidad de conseguir el mismo objetivo de diferentes formas, el problema de la redundancia para el investigador es qué elección tomar. Los controladores óptimos por realimentación resuelven la redundancia en línea y obedecen al principio de \emph{mínima intervención}, el cual establece que~\cite{Todorov2002}: ``no se debe realizar esfuerzo en corregir desviaciones del comportamiento esperado o promedio, a menos que las desviaciones interfieran con el desempeño de la tarea''. Una ley de control que obedezca el principio de mínima intervención, tiene el efecto de empujar \emph{el vector de estados}, de forma ortogonal a las direcciones de redundancia. Es importante mencionar que en el trabajo de~\cite{Todorov2004}, se resalta los resultados obtenidos por el control óptimo utilizando el lazo sensoriomotor, en donde las reducciones de dimensionalidad obtenidas por las sinergias musculares observadas en un EMG, son demostradas inicialmente mediante métodos de PCA. La emergencia de sinergias y variabilidad estructurada son obtenidas por un controlador neuronal, basado en control óptimo realimentado, en donde un espacio de estados de dimensión doce es estimado, para luego ser reducido a dos características relevantes de tareas, las cuales generan las salidas al espacio de actuación de dimensión cinco.

Las funciones del lazo sensoriomotor resultan de múltiples lazos de realimentación que operan simultáneamente, ajustando la rigidez y amortiguación muscular, proveen una realimentación instantánea~\cite{Todorov2004}. La medula espinal genera realimentaciones neurales rápidas; sin embargo lazos lentos resultan más adaptables y son implementados en la corteza motora. La diferencia de latencias puede ser organizada de más lentas pero más inteligentes, para responder a perturbaciones que los lazos rápidos no son capaces de atender. La organización jerárquica utiliza un nivel inferior de realimentación neural, que aumenta o transforma la dinámica del sistema músculo-esquelético, de forma tal que los niveles superiores ven un sistema dinámico más fácil de controlar óptimamente~\cite{WeiweiLi2004}. Una forma de construir un apropiado nivel inferior es mediante \emph{aprendizaje no supervisado}, el cual captura las regularidades estadísticas presentes en el flujo de comandos motores y datos sensoriales. Otro enfoque es utilizando el principio de intervención mínima, en donde, si se conocen las características que un controlador óptimo deba tener para una tarea especifica, entonces se puede diseñar un controlador de bajo nivel que extraiga esas características y las envíe a un nivel superior, en donde mapee los comandos musculares apropiados. Los esfuerzos computacionales de un controlador jerárquico son tan solo una fracción del cómputo requerido por uno no jerárquico, además los desempeños de un control jerárquico son aproximadamente óptimos. A parte de las características de las funciones del lazo sensoriomotor, las arquitecturas jerárquicas prometen ser útiles en el control en tiempo real de sistemas complejos, como prótesis robóticas, así como también, para estimuladores eléctricos implantados en varios músculos.

Continuando con las ideas de MC~\cite{Pfeifer2007}, en \cite{Ruckert2012} el propósito principal es, cómo cuantificar la cantidad de control que el sistema robótico le delega a la morfología. Para esto una cadena serial de cuatro eslabones, es utilizada para modelar el balance de un humanoide aplicando torque en diferentes articulaciones (tobillos, rodillas, cadera y brazos), además el balance se debe mantener en presencia de empujones externos. Los parámetros que forman parte de la morfología, los cuales son explorados son: la fricción de articulación, la longitud de los eslabones e inspirado por el trabajo de~\cite{Hauser2012}, se utilizaron resortes lineales cuyo acople con los eslabones permitían un comportamiento no-lineal en el torque de la articulación. Se utilizó CMA-ES para la búsqueda de la morfología. La idea principal es: (1) utilizar el \emph{Control Óptimo Estocástico} (SOC) para proponer leyes de control sobre cualquier morfología y (2) a partir de esto cambiar la morfología con el fin de reducir la complejidad de la señal de control y lograr un alto desempeño, obteniendo así un control y morfología óptimos. Acá la variación de las ganancias de control es relacionada con la cantidad de cómputo del controlador. Generalmente el problema SOC~\cite{Toussaint2009}, es resuelto para casos lineales mediante LQR, cuando el problema tiene incertidumbres se utiliza LQG, pero cuando el problema es además no-lineal, se puede utilizar iLQG~\cite{Todorov2005a}, que puede ser aplicado para problemas del caso LQG. En el trabajo de~\cite{Ruckert2012}, el SOC es planteado mediante un modelo gráfico, consistente en una red bayesiana causal, el método es denominado \emph{Control de inferencia aproximada} (AICO)~\cite{Toussaint2009} y puede ser extendido a casos no-LQG, en donde el modelo dinámico del sistema y la función de costo son representados por el modelo gráfico, la inferencia de la red bayesiana resulta en la política de control.

El siguiente trabajo de Rückert~\cite{Ruckert2012a}, utiliza el modelo gráfico anteriormente mencionado, para plantear un modelo de primitivas de movimiento (MP) capaz de proveer al sistema con un mecanismo de planeación probabilística, denominado \emph{Primitivas de movimiento de planeación} (PMP). Los aportes del PMP son: (1) al igual que los DMP aporta modularidad al sistema, (2) mejorando a los DMP, aporta optimalidad estocástica ante perturbaciones y (3) tiene eficiencia para el aprendizaje. Como novedad al trabajo anterior el modelo gráfico, el modelo dinámico y la función de costo son aprendidos mediante RL. Este trabajo involucra los tres tipos de aprendizaje RL: basado en modelo, libre de modelo y aprendizaje de modelo. A diferencia de los DMP que parametrizan las trayectorias codificándolas como atractores dinámicos, los PMP parametrizan la función de costo intrínseca del SOC, que difiere de la función de recompensa, para buscar políticas mediante un mecanismo de inferencia, luego probarlas en el modelo real, utilizar la información del modelo real para aproximar el modelo dinámico, y con el modelo dinámico interno realizar planeación sin requerir de un re-aprendizaje. En este trabajo se muestra la importancia de definir una tarea mediante características relevantes para facilitar el aprendizaje y generalización de habilidades de movimiento complejas.

En \cite{Ruckert2013}, se utiliza una modificación del \emph{framework} de los DMPs~\cite{Ijspeert2013}, que inspirado por hipótesis sobre el sistema de control neuromuscular implementa sinergias en las señales de control con el fin de reducir la dimensión del problema~\cite{DAvella2003}, acelerar el aprendizaje de tareas nuevas y reutilizar el conocimiento adquirido de otras tareas al estructurar el controlador en dos tipos de parámetros: (1) un conjunto de parámetros independientes de tareas, que representan señales de grupos musculares que forman funciones bases, (2) las funciones base o sinergias, se pueden correr en el tiempo y amplificar en magnitud mediante otro conjunto más pequeño de parámetros que es utilizado para desarrollar una tarea específica. Este método denominado \emph{DMPSynergies} se destaca en que, es más eficiente para el aprendizaje y permite aprendizaje multitarea de forma simultánea al compararse con los DMPs. Las sinergias son formadas a través del RL y permite estructurar las sinergias en su complejidad al estudiar de forma incremental la composición de las sinergias, definiendo así una medida de la complejidad del controlador. Las no-linealidades sobre la política complican el aprendizaje mediante imitación comparado con los DMPs. En este trabajo de Rückert se utiliza el modelo NMS basado de un brazo utilizando unidades de Hill, realizando tareas de alcance en donde se muestra una representación esparcida de las señales de control que coinciden con los datos de actividad electromiográfica y la habilidad de las sinergias para la generalización.

%Elmar Rückert, Calanca y Peters, modelo PMP, redes bayesianas.

Nihat Ay~\cite{Ay2012}, comenta cómo la teoría de la información ha sido utilizada recientemente para el estudio de la dinámica del lazo sensoriomotor de los robots y los seres vivos, tomándolos como sistemas de procesamiento de información, sobre los cuales puede formarse mecanismos de curiosidad e innovación. El problema principal es, como utilizar la información sensorial que produce las acciones del robot al interactuar con el entorno, de forma que se pueda afectar las acciones y percepciones futuras. La información predictiva (PI), es propuesta para cuantificar la totalidad de la información de las experiencias pasadas, que puede ser usada para predecir los eventos futuros. Con ejemplos lineales se concluye que el principio de la \emph{maximización de la información predictiva} (PIMAX) es una herramienta versátil para la auto-organización de comportamientos en sistemas robóticos complejos. La adaptación de los controladores, es decir la maximización de la información predictiva, se puede realizar mediante procesos de evolución o desarrollo, conduciendo a dos formas de representación: evolutiva y de aprendizaje en línea.

Continuando el trabajo anterior~\cite{Ay2012} y el trabajo de homeokinesis de~\cite{Der2012}, ~\cite{Martius2013} aplica ahora los resultados para modelos no-lineales y no-estacionarios, introduce el concepto de \emph{información predictiva de tiempo local} (TiPI), además las reglas de actualización de los parámetros del controlador, son planteadas de forma que, el principio de maximización de la información queda encarnado en las dinámicas sinápticas del sistema. El TiPI es demostrado sobre simulaciones físicas de sistemas de alta dimensionalidad, los cuales logran encarnación, motivación intrínseca y espontaneidad\footnote{Relacionado con auto-determinación}. Las no-linealidad producen en los comportamientos, cooperación simultánea y \emph{auto-switching} de dinámicas en sistemas con histéresis simples. Posterior a este trabajo, se prueba PIMAX sobre una plataforma robótica real utilizando TiPI~\cite{Martius2014}. Estos trabajos muestran como PI es un buen candidato para el aprendizaje ilimitado y autónomo de comportamientos explorando la dependencia del controlador a la morfología y el entorno~\cite{Zahedi2013}. 

Nihat Ay plantea el lazo sensoriomotor mediante un modelo gráfico (o red bayesiana) causal, en donde los fenómenos estocásticos de la interacción de un agente y su entorno, pueden ser modelados y las políticas o estrategias de control, pueden ser buscadas mediante el aprendizaje por refuerzo y el control óptimo estocástico, pero donde la información que fluye a través del lazo debe de ser organizada~\cite{Ay2015}. Las distribuciones presentes en el modelo gráfico son aproximadas empleando una \emph{Maquina de Boltzmann Restringida Condicional} (CRBM), en donde se explora la complejidad de la estructura morfológica dando criterios para construir la inteligencia encarnada con el mínimo de la arquitectura necesaria, basándose también en la relación de masa entre el cerebro y el cuerpo, tal y como se encuentra en los seres vivos, relación que también es proporcional al consumo de energía~\cite{Montufar2015}. Los trabajos anteriores están fundados sobre la cuantificación del MC~\cite{Zahedi2013a}.

%Exploración de redes bayesianas.

\section{Cognición, aprendizaje y toma de decisiones}
\label{sec:cognition}

Como se resume en~\cite{Zahedi2010}, la cognición de un sistema, hace referencia al proceso que transforma los datos sensoriales en comandos motores usando alguna forma de representación interna no-simbólica, y por lo tanto la cognición es un proceso el cual habita en el lazo sensoriomotor produciendo adaptación y desarrollo al sistema que lo contiene. En ese trabajo se buscó una regla de aprendizaje no-supervisado y auto-organizado general, que utilizara el lazo sensoriomotor, de forma que el agente no requiriera del conocimiento de un modelo estructural y suposiciones previas del entorno y la morfología, dicha regla esta basada sobre el principio de la maximización de la información.

Se extienden los resultados de~\cite{Martius2013} para RL en~\cite{Zahedi2013}, en donde los mecanismos de aprendizaje de comportamientos basados en PI, se utilizaron como \emph{motivación intrínseca} impulsada por información del lazo sensoriomotor que \emph{no es dependiente de una tarea especifica}\footnote{task-independent}, para proponer un aprendizaje que si es dirigido a la realización de una tarea específica, es decir \emph{dependiente de tareas}\footnote{task-dependent y task-independent. Trabajo similar al ya mensionado de las \emph{DMPSynergies}~\cite{Ruckert2013}}. Se combinan la señal de PI como motivación intrínseca (IRF), con una señal de motivación extrínseca (ERF), utilizando un algoritmo de búsqueda de políticas episódico.

%Lazo sensorio motor y teoría de la información, mediadas de información, cantidad mínima de información requerida para que un agente pueda maximizar una función de utilidad, reglas para el aprendizaje autónomo y no-supervisado.

En~\cite{Kober2013}, el RL es reseñado para los sistemas robóticos y es definido como \emph{RL Robótico} (RRL), mostrando sus principales retos y diferencias con: (1) el RL procedente del aprendizaje de máquina, en donde el problema basado o no en el modelo, obtiene de forma computacional los ensayos de prueba y error, además es usual que los espacios de acciones y estados sean discretos; y (2) el control óptimo estocástico, basado principalmente en el modelo del sistema\footnote{model-based} y los algoritmos basados en LQG~\cite{Toussaint2009}, donde una planta real hace más difícil la obtención de los ensayos de prueba y error. Tanto en RL, como en Control Óptimo, el problema fundamental a resolver es el mismo. Existen dos formas de planteamiento: el problema original, que mediante búsqueda de políticas frecuentemente basadas en la generación de trayectorias, encuentra la estrategia de control respetando un problema de optimización, definido por una función de costo sobre un espacio de estado-acción; Y el problema en su formulación dual, utilizando funciones de valor que aproximan el retorno o recompensa total, mediante las ecuaciones de Bellman. Obtenida esa aproximación se puede encontrar la política. Herramientas como el cálculo de variaciones, la programación dinámica, los métodos de Monte Carlo, la inferencia probabilística, la teoría de la información, los métodos de optimización y otros más, son utilizados para resolver el problema desde su formulación fundamental o dual.

Es importante tener presente que las técnicas tradicionales de RL probablemente estén condenadas a fallar en el RRL, por las consecuencias computacionales y del manejo de la información presentes en el RL robótico~\cite{Kober2013}. Los retos del RRL son más específicos que en el RL, muchos de estos retos hacen el problema RRL intratable, ejemplos de ello son: el muestreo del mundo real, la especificación del objetivo, la maldición de la dimensionalidad, el dilema de exploración-explotación, el sub-modelado del problema real, el modelado de incertidumbres, la selección de características, la transferencia de soluciones, los modelos y las políticas estructurados; que son algunos retos notablemente diferentes del RL tradicional. A continuación se mencionan algunos:

%Los sistemas robóticos no se pueden considerar como completamente observables o libres de ruido, es decir no es un problema definido como \emph{Proceso de Decisión de Markov} (MDP) determinístico, en realidad son casi un POMDP estocástico (es decir MDPs parcialmente observables y estocásticos). No obstante son un poco más complejos ya que la propiedad markoviana, no se asegura por los retardos en las señales de actuación y sensado. Los sistemas de aprendizaje robóticos utilizan filtros para estimar el estado real. Es esencial que, la información del estado tenga además del valor sensado, una noción de incertidumbre sobre sus estimados. Esto hace que la experiencia sobre un sistema real sea difícil y costosa de obtener y de reproducir. Además, a diferencia del RL tradicional en donde la señal de recompensa frecuentemente es \emph{sparse}, el diseño de la recompensa en el RL robótico, denominado \emph{reward shaping}, que se utiliza para codificar el objetivo y acelerar exitosamente el aprendizaje, requiere de una gran cantidad de conocimiento en el dominio de aplicación para su implementación, lo cual hace difícil su práctica. 
\begin{itemize}
\item Los sistemas robóticos no se pueden considerar como
  completamente observables o libres de ruido, no obstante son un poco
  más complejos que los POMDPs estocásticos, ya que la propiedad
  markoviana no se asegura por los retardos en las señales de
  actuación y sensado. Los sistemas de aprendizaje robóticos utilizan
  filtros para estimar el estado real. Es esencial que, la información
  del estado tenga además del valor sensado, una noción de
  incertidumbre sobre sus estimados. Esto hace que la experiencia
  sobre un sistema real, sea difícil, costosa de obtener y de
  reproducir.

\item Además de la señal de sensado, la señal de recompensa, que se
  utiliza para codificar el objetivo y acelerar exitosamente el
  aprendizaje, requiere de un diseño cuidadoso. Al diseño de la señal de
  recompensa se le denomina \emph{reward shaping}. A diferencia del RL
  tradicional donde la señal, con frecuencia es \emph{sparse}, en el RRL se requiere
  de una gran cantidad de conocimiento en el dominio de aplicación
  para su implementación, lo cual hace difícil su práctica.

\item El problema de \emph{la maldición de la dimensionalidad}, que debido
  a los sistemas robóticos humanoides, fácilmente alcanzan 100 DoFs\footnote{DoF: grados de libertad}, y
  en donde el espacio estado-acción es continuo, hacen nuevamente al
  RL robótico intratable.

  % Los métodos \emph{on-policy} recolectan la información usando una
  % política actual, y como resultado la exploración se construye
  % sobre la política y esta determina la velocidad de mejoramiento de
  % la política, obteniendo mejoramientos a corto y largo plazo.
\item El problema entre \emph{exploración y explotación}, el cual permite
  explorar el espacio estado-acción, puede ser intratable y dedicarse
  solo a explotación, puede conducir a óptimos locales, los cuales
  pueden resultar en desempeños de controladores pobres. Modelar la
  exploración mediante modelos probabilísticos permiten la
  tratabilidad de la alta dimensionalidad, además hace que el paso de
  mejoramiento de la política sea sencillo de implementar.

\item El problema de \emph{la eficiencia estadística}, en RRL, se considera más importante en limitar el tiempo de interacción con el mundo real en lugar de limitar el consumo de memoria y la complejidad computacional. Por lo tanto los algoritmos de eficiencia muestral, que son capaces de aprender con un pequeño número de intentos o corridas es esencial, ya los fallos en \emph{hardware} son costos en tiempo y dinero.
\end{itemize}

Los métodos basados en \emph{búsqueda de políticas} (PS) permiten de forma natural la integración del conocimiento, que a su vez permite la pre-estructuración apropiada para un dominio especifico de la forma aproximada de la política, sin cambiar el problema original~\cite{Kober2011}. Además permite incorporar de forma natural restricciones adicionales al problema~\cite{Kober2013}. 

El uso de funciones de aproximación procedentes de los método de aprendizaje supervisado, están diseñados bajo la suposición de que las muestras de datos son \emph{independientes e idénticamente distribuidas} (\emph{i.i.d}), éstas muestras enfrentan en el RRL, que los datos proceden de los sistemas robóticos y no son ni independientes y tampoco idénticamente distribuidos. El uso de \emph{minibatches} al interior de cada iteración y la reutilización de las muestras se propone en~\cite{Heess2015,Lillicrap2015}, para poder hacer uso adecuado de las técnicas de aprendizaje supervisado.

%Las restricciones pueden limitar el espacio de búsqueda pero también involucran problemas adicionales, que generalmente se relajan en los métodos de optimización de la búsqueda, pero aquí esta no es una opción. 
%Para la tratabilidad de los retos del problema RL robótico se utiliza: a través de la representación, a través de la aproximación de modelos, el manejo de conocimiento a-priori e información. El conocimiento a-priori puede ser incluido mediante: políticas iniciales, demostraciones, modelos iniciales, estructuras de tarea predefinidas, o restricciones sobre políticas (e.g. como limites de torque), con el fin de reducir los espacios de búsqueda y acelerar el proceso de aprendizaje. Pre-estructurar una tarea compleja en pequeños problemas más sencillos, puede reducir la complejidad de la tarea de aprendizaje. 

Una estructura de RL jerárquico utilizando meta-acciones, que corresponde al manejo de tareas de bajo nivel mediante una capa de alto nivel que coordina las acciones motoras mediante estrategias, por ejemplo, el ponerse de pie se obtiene mediante dos capas: la primera, \emph{Q-learning} o \emph{SARSA} en la capa de estrategia; y la segunda, con \emph{actor-critic} en el bajo nivel, como en~\cite{Morimoto2001}. Estructuras de control similares~\cite{Sabourin2005,Madani2011} a la de \citeasnoun{Morimoto2001}, han sido el inicio de estructuras bio-inspiradas del CNS. Este RL jerárquico ha sido empleado con éxito en el controladores de marcha de humanoides, encontrando controladores óptimos a partir de coordinar subsistemas más simples. Inspirados por los sistemas biológicos, que aprenden de forma progresiva el desarrollo de sus tareas, se utiliza el \emph{reward shaping} de tal forma que el término de balance domine sobre los otros términos, como los de orientación a un objetivo, y por lo tanto se logra que el balance se aprenda en primera instancia~\cite{Kober2013}. 

%El conocimiento a-priori de la tarea para mecanismos de curiosidad que exploren regiones nuevas y prometedoras. 
Muchos problemas de RRL pueden ser tratables al aprender \emph{modelos directos}~\footnote{forward model}, que son modelos dinámicos de transición que se soportan sobre los datos y permiten entrenar los sistemas con una mejor interacción con el entorno real. Combinar el aprendizaje de simulación y el sistema real para reducir la experimentación real se conoce como \emph{mental rehearsal} o \emph{ensayos metales}~\cite{Kober2013}. Un problema frecuente de los ensayos mentales, es el \emph{sesgo de simulación}\footnote{\emph{Simulation Biases}}, que surge de la imposibilidad de obtener un modelo directo, para simular con precisión el sistema robótico real sin error alguno, resultando en políticas que funcionan bien en el modelo directo pero pobremente en el sistema real. Promediar sobre el modelo de incertidumbre en un modelo probabilístico puede ser usado para reducir el \emph{bias}. Frecuentemente introducir modelos estocásticos o distribuciones sobre los modelos, permite enfrentar el sesgo de simulación, un ejemplo es~\cite{Calandra2015}, en donde se aproxima un modelo dinámico con contacto, allí, añadir ruido artificial a las señales permite suavizar los errores del modelo y evadir el sobre-ajuste de la política. Los métodos de aprendizaje de modelo, que manejan una incertidumbre probabilística alrededor de la verdadera dinámica del sistema permiten al algoritmo de RL generar distribuciones sobre el desempeño de la política, en \cite{Calandra2014a,Calandra2014,Calandra2016} se evalúa sobre una plataforma robótica bípeda, el aprendizaje del modo de andar, empleando modelos optimización bayesiana para el manejo de incertidumbres. 

Por último, en~\cite{Kober2013} se destaca más aspectos del ensayo mental: (1) Muestreando y re-utilizando números aleatorios, un modelo directo puede ser usado en simulación para crear \emph{roll-outs} para entrenar por muestreo. (2) Cuando se compara los resultados de diferentes corridas de simulación, es realmente difícil decir, el número más pequeño de muestras para que una política trabaje mejor o solo sea el efecto de la estocasticidad del modelo simulado. (3) Aprender una política por lo general es más sencillo que aprender un modelo directo y por lo tanto los métodos libres de modelo, son más usados en la práctica que los métodos basados en modelo, ya que estos a partir de los datos aprenden un modelo de la dinámica del robot y con este modelo mediante simulación se obtiene las políticas o el mejoramiento de las mismas para el sistema real, con lo cual se requiere una implementación más compleja.

% Al tratar la incertidumbre como si fuera ruido y empleando una aproximación analítica de una simulación directa, la tarea de elevar un cart-pole puede ser resuelta en menos de 20 segundos de interacción con el modelo real.

El último, los trabajos a citar en este párrafo, son ejemplos de trabajos desarrollados que cumplen parcial o totalmente con las proposiciones que definen el problema de investigación. De la computación gráfica, en el campo de la animación de personajes, fundamentados por los trabajos de IRL y SOC (1) de Emo Todorov, (2) de Sergei Levine añadiendo las ventajas del Deep Learning. Recientemente el trabajo de van de Panne~\cite{Agrawal2016,Liu2016,Peng2016a}\cite{Geijtenbeek2013}. El trabajo de~\cite{Holden2016} utilizan el Deep Learning, el Deep RL y el Deep RL inverso, para proponer controladores de personajes humanoides. Las redes DNN para el control de movimientos, de aproximación directa de la política.

% La Cinemática Inversa (IK) No Requiere de ser calculada bajo la lógica morfológica~\cite{Li2013a},

%Representación y selección de características para la locomoción y el comportamiento coordinado. Definición de Aprendizaje de Maquina.

%Introducción del Deep Learning en RL. Trabajos relevantes.

%Distributed representation and factors of variation.

